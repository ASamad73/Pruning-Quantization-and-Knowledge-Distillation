{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T18:21:25.419579Z",
     "iopub.status.busy": "2025-11-01T18:21:25.419103Z",
     "iopub.status.idle": "2025-11-01T18:21:25.424840Z",
     "shell.execute_reply": "2025-11-01T18:21:25.423900Z",
     "shell.execute_reply.started": "2025-11-01T18:21:25.419554Z"
    },
    "id": "nRLKQwiGy84s",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os, time, json\n",
    "import copy\n",
    "import math\n",
    "import argparse\n",
    "from collections import OrderedDict, defaultdict\n",
    "from typing import Dict, Tuple, List, Any\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.serialization import add_safe_globals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "friDQYx-3_Fx"
   },
   "source": [
    "**Task 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-11-01T18:21:28.127066Z",
     "iopub.status.busy": "2025-11-01T18:21:28.126808Z",
     "iopub.status.idle": "2025-11-01T18:21:31.706095Z",
     "shell.execute_reply": "2025-11-01T18:21:31.705398Z",
     "shell.execute_reply.started": "2025-11-01T18:21:28.127049Z"
    },
    "id": "uQucu9G24VO0",
    "outputId": "2eee757c-f5f1-454c-dcc1-d7fa53a59961",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU(inplace=True)\n",
       "    (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2470, 0.2435, 0.2616])\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "VAL_SIZE = 5000\n",
    "TEST_SIZE = len(CIFAR10_testset) - VAL_SIZE\n",
    "\n",
    "CIFAR10_trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "CIFAR10_testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "CIFAR10_valset, CIFAR10_testset_final = random_split(CIFAR10_testset, [VAL_SIZE, TEST_SIZE], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "CIFAR10_trainloader = DataLoader(CIFAR10_trainset, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "CIFAR10_valloader = DataLoader(CIFAR10_valset, batch_size=64, shuffle=False, num_workers=2)\n",
    "CIFAR10_testloader = DataLoader(CIFAR10_testset_final, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "CIFAR10_classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "num_classes = len(CIFAR10_classes)\n",
    "\n",
    "vgg_model = torchvision.models.vgg11_bn(pretrained=False)\n",
    "vgg_model.classifier[6] = nn.Linear(in_features=vgg_model.classifier[6].in_features, out_features=num_classes)\n",
    "vgg_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VFZyZLj-4BPv"
   },
   "source": [
    "*Unstructured Pruning*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T18:22:05.606598Z",
     "iopub.status.busy": "2025-11-01T18:22:05.606220Z",
     "iopub.status.idle": "2025-11-01T18:22:05.614063Z",
     "shell.execute_reply": "2025-11-01T18:22:05.613287Z",
     "shell.execute_reply.started": "2025-11-01T18:22:05.606577Z"
    },
    "id": "Cv8tNaDb3-fx",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_prunable_param_names(model):\n",
    "    prunable = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.endswith(\".weight\"):\n",
    "            comps = name.split(\".\")\n",
    "            module = model\n",
    "            for comp in comps[:-1]:\n",
    "                if hasattr(module, comp):\n",
    "                    module = getattr(module, comp)\n",
    "                else:\n",
    "                    try:\n",
    "                        idx = int(comp)\n",
    "                        module = module[idx]\n",
    "                    except Exception:\n",
    "                        module = None\n",
    "                        break\n",
    "            if module is not None:\n",
    "                if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "                    prunable.append(name)\n",
    "    return prunable\n",
    "\n",
    "def init_masks(model, prunable_names):\n",
    "  masks = {}\n",
    "  for name, param in model.named_parameters():\n",
    "    if name in prunable_names:\n",
    "      masks[name] = torch.ones_like(param.data, dtype=torch.uint8).cpu()\n",
    "  return masks\n",
    "\n",
    "def apply_masks(model, masks):\n",
    "  with torch.no_grad():\n",
    "    for name, param in model.named_parameters():\n",
    "      if name in masks:\n",
    "        mask = masks[name].to(param.device).to(param.dtype)\n",
    "        param.data.mul_(mask)\n",
    "\n",
    "def count_global_sparsity(masks):\n",
    "  total = sum(m.numel() for m in masks.values())\n",
    "  nonzero = sum(int(m.sum().item()) for m in masks.values())\n",
    "  return 1.0 - (nonzero/total), nonzero, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T18:22:07.242976Z",
     "iopub.status.busy": "2025-11-01T18:22:07.242441Z",
     "iopub.status.idle": "2025-11-01T18:22:07.249067Z",
     "shell.execute_reply": "2025-11-01T18:22:07.248268Z",
     "shell.execute_reply.started": "2025-11-01T18:22:07.242952Z"
    },
    "id": "Qoe6GXCn5Rnx",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def prune_layer_by_magnitude(model, masks, layer_name, target_sparsity):\n",
    "  assert 0.0 <= target_sparsity <= 1.0\n",
    "  param = dict(model.named_parameters())[layer_name]\n",
    "  w = param.data.detach().cpu().clone()\n",
    "  mask = masks[layer_name].cpu().clone()\n",
    "  total = mask.numel()\n",
    "  target_pruned = int(math.floor(target_sparsity * total))\n",
    "  currently_pruned = int((mask==0).sum().item())\n",
    "  need_prune = max(0, target_pruned - currently_pruned)\n",
    "  if need_prune == 0:\n",
    "    return\n",
    "  active_flat_idx = torch.nonzero(mask.view(-1)).squeeze()\n",
    "  if active_flat_idx.numel() == 0:\n",
    "    return\n",
    "  active_vals = w.view(-1).abs()[active_flat_idx]\n",
    "  if need_prune >= active_vals.numel():\n",
    "    new_mask = torch.zeros_like(mask)\n",
    "    masks[layer_name] = new_mask.to(torch.uint8)\n",
    "    return\n",
    "  _, order = torch.sort(active_vals)\n",
    "  selected = active_flat_idx[order[:need_prune]]\n",
    "  new_mask_flat = mask.view(-1).clone()\n",
    "  new_mask_flat[selected] = 0\n",
    "  masks[layer_name] = new_mask_flat.view(mask.shape).to(torch.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T18:22:08.490547Z",
     "iopub.status.busy": "2025-11-01T18:22:08.489967Z",
     "iopub.status.idle": "2025-11-01T18:22:08.497276Z",
     "shell.execute_reply": "2025-11-01T18:22:08.496541Z",
     "shell.execute_reply.started": "2025-11-01T18:22:08.490524Z"
    },
    "id": "yUkt3Bpl8opv",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer, masks=None):\n",
    "  model.train()\n",
    "  running_loss = 0.0\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  for images, labels in dataloader:\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(images)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if masks is not None:\n",
    "      apply_masks(model, masks)\n",
    "    batch_size = images.size(0)\n",
    "    running_loss += loss.item() * batch_size\n",
    "    _, preds = outputs.max(1)\n",
    "    correct += preds.eq(labels).sum().item()\n",
    "    total += batch_size\n",
    "  return running_loss / total, 100.0 * correct / total\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader):\n",
    "  model.eval()\n",
    "  total = 0\n",
    "  correct = 0\n",
    "  for images, labels in dataloader:\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    outputs = model(images)\n",
    "    _, preds = outputs.max(1)\n",
    "    correct += preds.eq(labels).sum().item()\n",
    "    total += images.size(0)\n",
    "  return 100.0 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T18:22:13.487897Z",
     "iopub.status.busy": "2025-11-01T18:22:13.487357Z",
     "iopub.status.idle": "2025-11-01T18:22:13.494680Z",
     "shell.execute_reply": "2025-11-01T18:22:13.493821Z",
     "shell.execute_reply.started": "2025-11-01T18:22:13.487872Z"
    },
    "id": "v-v3KhtH9WA2",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_layer_histogram(model, masks, layer_name, out_dir, tag):\n",
    "  os.makedirs(out_dir, exist_ok=True)\n",
    "  w = dict(model.named_parameters())[layer_name].data.detach().cpu().numpy().flatten()\n",
    "  m = masks[layer_name].cpu().numpy().flatten()\n",
    "  active = w[m==1]\n",
    "  plt.figure(figsize=(6,4))\n",
    "  plt.hist(w, bins=120, alpha=0.4, label='all', density=True)\n",
    "  if active.size>0:\n",
    "    plt.hist(active, bins=120, alpha=0.6, label='active (mask=1)', density=True)\n",
    "  plt.title(f\"{layer_name} ({tag})\")\n",
    "  plt.legend()\n",
    "  plt.tight_layout()\n",
    "  fname = os.path.join(out_dir, f\"{layer_name.replace('.','_')}_{tag}.png\")\n",
    "  plt.savefig(fname)\n",
    "  plt.close()\n",
    "\n",
    "def save_all_histograms(model, masks, prunable_names, out_dir, tag):\n",
    "  for name in prunable_names:\n",
    "    save_layer_histogram(model, masks, name, out_dir, tag)\\\n",
    "\n",
    "def display_figure(fig):\n",
    "  try:\n",
    "    from IPython.display import display\n",
    "    ipy_available = True\n",
    "  except ImportError:\n",
    "      ipy_available = False\n",
    "\n",
    "  try:\n",
    "      if ipy_available and fig is not None:\n",
    "          display(fig)\n",
    "      else:\n",
    "          plt.show()\n",
    "  except Exception:\n",
    "      plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T18:22:15.428061Z",
     "iopub.status.busy": "2025-11-01T18:22:15.427307Z",
     "iopub.status.idle": "2025-11-01T18:22:15.437914Z",
     "shell.execute_reply": "2025-11-01T18:22:15.436952Z",
     "shell.execute_reply.started": "2025-11-01T18:22:15.428034Z"
    },
    "id": "FKWgd-6M9jLU",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_sensitivity(baseline_checkpoint, prunable_names, dataloaders, device, sparsity_list, short_finetune_epochs=3, out_dir='sensitivity'):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    trainloader, testloader = dataloaders\n",
    "\n",
    "    model = torchvision.models.vgg11_bn(pretrained=False)\n",
    "    model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n",
    "    ck = torch.load(baseline_checkpoint, map_location='cpu')\n",
    "    model.load_state_dict(ck['model_state_dict'])\n",
    "    model.to(device)\n",
    "\n",
    "    baseline_acc = evaluate(model, testloader)  \n",
    "    print(\"Baseline accuracy (loaded):\", baseline_acc)\n",
    "\n",
    "    results = {}\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for layer in prunable_names:\n",
    "        results[layer] = {'sparsities': [], 'acc_no_ft': []}\n",
    "        for s in sparsity_list:\n",
    "            sp = float(s) / 100.0\n",
    "            model.load_state_dict(ck['model_state_dict'])\n",
    "            model.to(device)\n",
    "            masks = init_masks(model, prunable_names)\n",
    "\n",
    "            prune_layer_by_magnitude(model, masks, layer, sp)\n",
    "            apply_masks(model, masks)\n",
    "\n",
    "            acc_no = evaluate(model, testloader)\n",
    "\n",
    "            results[layer]['sparsities'].append(s)\n",
    "            results[layer]['acc_no_ft'].append(acc_no)\n",
    "            print(f\"[sens] {layer} @ {s}% -> no_ft {acc_no:.2f}\")\n",
    "\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.plot(results[layer]['sparsities'], results[layer]['acc_no_ft'], marker='o', label='no finetune')\n",
    "        plt.xlabel('Sparsity (%)'); plt.ylabel('Val Accuracy (%)'); plt.title(layer)\n",
    "        plt.legend(); plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(out_dir, f\"{layer.replace('.','_')}_sensitivity.png\"))\n",
    "        plt.close()\n",
    "\n",
    "    np.save(os.path.join(out_dir, 'sensitivity_results.npy'), results)\n",
    "    print(\"Saved sensitivity results to\", out_dir)\n",
    "\n",
    "    avg_acc_drop = {}\n",
    "    for layer, rec in results.items():\n",
    "      base = baseline_acc\n",
    "      mean_no_ft = np.mean(rec['acc_no_ft'])\n",
    "      avg_acc_drop[layer] = base - mean_no_ft\n",
    "    sorted_layers = sorted(avg_acc_drop.items(), key=lambda x: x[1])  \n",
    "    ranked_path = os.path.join(out_dir, 'ranked_layers_by_sensitivity.json')\n",
    "    json.dump(sorted_layers, open(ranked_path, 'w'), indent=2)\n",
    "    print(f\"Saved layer ranking (least sensitive first) → {ranked_path}\")\n",
    "\n",
    "    return results, baseline_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T18:22:44.554630Z",
     "iopub.status.busy": "2025-11-01T18:22:44.554319Z",
     "iopub.status.idle": "2025-11-01T18:22:44.566975Z",
     "shell.execute_reply": "2025-11-01T18:22:44.565867Z",
     "shell.execute_reply.started": "2025-11-01T18:22:44.554608Z"
    },
    "id": "aWwM_R6OhLg7",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_baseline(baseline_path=\"baseline_vgg11.pth\",\n",
    "                   epochs=20,\n",
    "                   lr=0.001,\n",
    "                   batch_size=64,\n",
    "                   momentum=0.9,\n",
    "                   weight_decay=5e-4):\n",
    "    vgg_model.to(device)\n",
    "    p = next(vgg_model.parameters())\n",
    "    print(\"Model param device:\", p.device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(vgg_model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        vgg_model.train()\n",
    "        total, correct, running_loss = 0, 0, 0.0\n",
    "        for images, labels in CIFAR10_trainloader:\n",
    "            images, labels = images.to(device), labels.to(device)   \n",
    "            optimizer.zero_grad()\n",
    "            outputs = vgg_model(images)                             \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, preds = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += preds.eq(labels).sum().item()\n",
    "\n",
    "        train_acc = 100.0 * correct / total\n",
    "        train_loss = running_loss / total\n",
    "\n",
    "        vgg_model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in CIFAR10_valloader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = vgg_model(images)\n",
    "                _, preds = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += preds.eq(labels).sum().item()\n",
    "        val_acc = 100.0 * correct / total\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}/{epochs}] Train Acc: {train_acc:.2f}%  Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            state_dict_cpu = {k: v.cpu() for k, v in vgg_model.state_dict().items()}\n",
    "            torch.save({\n",
    "                'model_state_dict': state_dict_cpu,\n",
    "                'num_classes': num_classes,\n",
    "                'arch': 'vgg11_bn'\n",
    "            }, baseline_path)\n",
    "            print(f\"Saved baseline clean state_dict -> {baseline_path}\")\n",
    "            print(f\"Saved new best model ({best_acc:.2f}%)\")\n",
    "\n",
    "    print(f\"Baseline training complete. Best validation accuracy: {best_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T18:22:20.894246Z",
     "iopub.status.busy": "2025-11-01T18:22:20.893984Z",
     "iopub.status.idle": "2025-11-01T18:22:20.953896Z",
     "shell.execute_reply": "2025-11-01T18:22:20.953149Z",
     "shell.execute_reply.started": "2025-11-01T18:22:20.894226Z"
    },
    "id": "pgQ5ACE1szEE",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import math, json, numpy as np\n",
    "\n",
    "def suggest_unstructured_allocation(prunable_names, sensitivity_results, target_global_sparsity=0.70, per_layer_cap=0.95, anchor_sparsity=50, baseline_acc=None):\n",
    "    name_to_param = {n: dict(vgg_model.named_parameters())[n] for n in prunable_names}\n",
    "    layer_param_counts = {n: name_to_param[n].numel() for n in prunable_names}\n",
    "    total_params = sum(layer_param_counts.values())\n",
    "    target_prune_params = int(round(target_global_sparsity * total_params))\n",
    "\n",
    "    scores = {}\n",
    "    for n in prunable_names:\n",
    "        res = sensitivity_results.get(n, None)\n",
    "        if res:\n",
    "            spars_list = res.get('sparsities', [])\n",
    "            accs_no = res.get('acc_no_ft', []) or res.get('acc_after_ft', [])\n",
    "            if len(spars_list) and len(accs_no):\n",
    "                idx = min(range(len(spars_list)), key=lambda i: abs(spars_list[i] - anchor_sparsity))\n",
    "                scores[n] = float(accs_no[idx])\n",
    "            else:\n",
    "                if accs_no:\n",
    "                    scores[n] = float(max(accs_no))\n",
    "                else:\n",
    "                    scores[n] = -1.0\n",
    "        else:\n",
    "            scores[n] = -1.0\n",
    "\n",
    "    if baseline_acc is None:\n",
    "        all_accs = []\n",
    "        for res in sensitivity_results.values():\n",
    "            all_accs += res.get('acc_no_ft', []) or res.get('acc_after_ft', []) or []\n",
    "        baseline_acc = float(max(all_accs)) if all_accs else None\n",
    "\n",
    "    items = sorted(prunable_names, key=lambda x: scores.get(x, -1.0), reverse=True)\n",
    "\n",
    "    to_prune = target_prune_params\n",
    "    allocated = {n: 0 for n in prunable_names}\n",
    "    for name in items:\n",
    "        if to_prune <= 0:\n",
    "            break\n",
    "        layer_total = layer_param_counts[name]\n",
    "        cap = int(math.floor(per_layer_cap * layer_total))\n",
    "        take = min(cap, to_prune)\n",
    "        allocated[name] = take\n",
    "        to_prune -= take\n",
    "\n",
    "    if to_prune > 0:\n",
    "        for name in items:\n",
    "            if to_prune <= 0:\n",
    "                break\n",
    "            layer_total = layer_param_counts[name]\n",
    "            cap = int(math.floor(per_layer_cap * layer_total))\n",
    "            avail = cap - allocated[name]\n",
    "            if avail <= 0:\n",
    "                continue\n",
    "            add = min(avail, to_prune)\n",
    "            allocated[name] += add\n",
    "            to_prune -= add\n",
    "\n",
    "    per_layer_sparsities = {}\n",
    "    for name in prunable_names:\n",
    "        frac = float(allocated.get(name, 0)) / float(layer_param_counts[name]) if layer_param_counts[name] > 0 else 0.0\n",
    "        per_layer_sparsities[name] = min(max(frac, 0.0), per_layer_cap)\n",
    "\n",
    "    est_pruned = sum(int(round(per_layer_sparsities[n] * layer_param_counts[n])) for n in prunable_names)\n",
    "    info = {\n",
    "        'total_params': int(total_params),\n",
    "        'target_prune_params': int(target_prune_params),\n",
    "        'estimated_pruned_params': int(est_pruned),\n",
    "        'estimated_global_sparsity': float(est_pruned / total_params) if total_params > 0 else 0.0,\n",
    "        'per_layer_sparsities': per_layer_sparsities,\n",
    "        'scores_anchor': {n: float(scores.get(n, -1.0)) for n in prunable_names},\n",
    "        'baseline_acc_used': float(baseline_acc) if baseline_acc is not None else None\n",
    "    }\n",
    "\n",
    "    print(f\"[allocator] total_params={total_params}, target_prune_params={target_prune_params}, estimated_pruned={est_pruned}, est_global_sparsity={info['estimated_global_sparsity']:.4f}\")\n",
    "    return per_layer_sparsities, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T18:22:23.682498Z",
     "iopub.status.busy": "2025-11-01T18:22:23.681789Z",
     "iopub.status.idle": "2025-11-01T18:22:23.691755Z",
     "shell.execute_reply": "2025-11-01T18:22:23.690919Z",
     "shell.execute_reply.started": "2025-11-01T18:22:23.682473Z"
    },
    "id": "SovQp5w-HRMb",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def apply_per_layer_and_finetune(baseline_checkpoint, per_layer_sparsities, dataloaders, device, finetune_epochs, out_dir='final_pruned'):\n",
    "\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    trainloader, testloader = dataloaders\n",
    "\n",
    "    model = torchvision.models.vgg11_bn(pretrained=False)\n",
    "    model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n",
    "    ckpt = torch.load(baseline_checkpoint, map_location='cpu')\n",
    "    model.load_state_dict(ckpt['model_state_dict'])\n",
    "    model = model.to(device)\n",
    "\n",
    "    prunable = get_prunable_param_names(model)\n",
    "    masks = {name: torch.ones_like(p, dtype=torch.uint8).cpu()\n",
    "             for name, p in model.named_parameters() if name in prunable}\n",
    "\n",
    "    for name, s in per_layer_sparsities.items():\n",
    "        if name not in masks:\n",
    "            print(f\"[WARN] Skipping {name}: not found or not prunable.\")\n",
    "            continue\n",
    "        prune_layer_by_magnitude(model, masks, name, s)\n",
    "\n",
    "    apply_masks(model, masks)\n",
    "    initial_acc = evaluate(model, testloader)\n",
    "    print(f\"Initial accuracy after applying sparsities: {initial_acc:.2f}%\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "    for epoch in range(finetune_epochs):\n",
    "        train_loss, train_acc = train_one_epoch(model, trainloader, criterion, optimizer, masks=masks)\n",
    "        val_acc = evaluate(model, testloader)\n",
    "        print(f\"[Finetune {epoch+1}/{finetune_epochs}] Train Acc={train_acc:.2f}%, Val Acc={val_acc:.2f}%\")\n",
    "\n",
    "    total_params, nonzero_params = 0, 0\n",
    "    for m in masks.values():\n",
    "        total_params += m.numel()\n",
    "        nonzero_params += (m == 1).sum().item()\n",
    "    global_sparsity = 1.0 - (nonzero_params / total_params)\n",
    "    print(f\"Final global sparsity: {global_sparsity*100:.2f}%\")\n",
    "\n",
    "    final_sd = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "    torch.save({\n",
    "        'model_state_dict': final_sd,\n",
    "        'num_classes': num_classes,\n",
    "        'arch': 'vgg11_bn',\n",
    "        'masks': {k: v.cpu().astype('uint8') if isinstance(v, np.ndarray) else v.cpu().numpy() for k,v in masks.items()},\n",
    "        'global_sparsity': global_sparsity\n",
    "    }, os.path.join(out_dir, 'final_pruned_state_dict.pth'))\n",
    "    print(\"Saved final unstructured state_dict:\", os.path.join(out_dir, 'final_pruned_state_dict.pth'))\n",
    "\n",
    "    return model, masks, global_sparsity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T18:22:26.789670Z",
     "iopub.status.busy": "2025-11-01T18:22:26.789371Z",
     "iopub.status.idle": "2025-11-01T18:22:26.798236Z",
     "shell.execute_reply": "2025-11-01T18:22:26.797450Z",
     "shell.execute_reply.started": "2025-11-01T18:22:26.789649Z"
    },
    "id": "OT_EdVgn96NA",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_unstructured_pipeline(baseline_checkpoint, sparsity_list_percent=[10, 30, 50, 70], short_finetune_epochs=3, final_finetune_epochs=15, out_dir='unstructured_outputs', auto_apply_suggestion=True):\n",
    "  \n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    if not os.path.exists(baseline_checkpoint):\n",
    "        raise FileNotFoundError(f\"Baseline checkpoint not found: {baseline_checkpoint}\")\n",
    "\n",
    "    ck = torch.load(baseline_checkpoint, map_location='cpu')\n",
    "    try:\n",
    "        vgg_model.load_state_dict(ck['model_state_dict'])\n",
    "    except Exception:\n",
    "        vgg_model.load_state_dict(ck, strict=False)\n",
    "    vgg_model.to(device)\n",
    "\n",
    "    prunable_names = get_prunable_param_names(vgg_model)\n",
    "\n",
    "    masks_before = init_masks(vgg_model, prunable_names)\n",
    "    save_all_histograms(vgg_model, masks_before, prunable_names, os.path.join(out_dir,'histograms_before'), tag='before')\n",
    "\n",
    "    trainloader, testloader = CIFAR10_trainloader, CIFAR10_testloader\n",
    "    print(\"Running sensitivity analysis (unstructured). This may take some time...\")\n",
    "    results, baseline_acc = run_sensitivity(baseline_checkpoint, prunable_names, (trainloader, testloader),\n",
    "                                           device, sparsity_list_percent, short_finetune_epochs,\n",
    "                                           out_dir=os.path.join(out_dir,'sensitivity'))\n",
    "    print(\"Sensitivity done. Baseline accuracy:\", baseline_acc)\n",
    "\n",
    "    per_layer_suggestion, info = suggest_unstructured_allocation(prunable_names, results, target_global_sparsity=0.70, anchor_sparsity=50)\n",
    "    suggestion_path = os.path.join(out_dir, 'suggested_unstruct_sparsities.json')\n",
    "    with open(suggestion_path, 'w') as f:\n",
    "        json.dump(per_layer_suggestion, f, indent=2)\n",
    "    print(f\"Saved suggested per-layer sparsities to {suggestion_path}\")\n",
    "\n",
    "    if not auto_apply_suggestion:\n",
    "        print(\"auto_apply_suggestion=False: returning suggestion path for manual editing.\")\n",
    "        return suggestion_path\n",
    "\n",
    "    pruned_model, masks_final, spars = apply_per_layer_and_finetune(baseline_checkpoint, per_layer_suggestion,\n",
    "                                                                   (trainloader, testloader), device,\n",
    "                                                                   finetune_epochs=final_finetune_epochs,\n",
    "                                                                   out_dir=os.path.join(out_dir,'final'))\n",
    "    \n",
    "    save_all_histograms(pruned_model, masks_final, prunable_names, os.path.join(out_dir,'histograms_after'), tag='after')\n",
    "    print(\"Final unstructured model saved. Global sparsity:\", spars)\n",
    "    return pruned_model, masks_final, spars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-11-01T18:22:52.276748Z",
     "iopub.status.busy": "2025-11-01T18:22:52.276028Z",
     "iopub.status.idle": "2025-11-01T18:34:38.255740Z",
     "shell.execute_reply": "2025-11-01T18:34:38.254728Z",
     "shell.execute_reply.started": "2025-11-01T18:22:52.276725Z"
    },
    "id": "f83a5vvOgEiP",
    "outputId": "97b269a9-a6ec-44f4-ee91-e8b0419cb71f",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model param device: cuda:0\n",
      "[Epoch 1/15] Train Acc: 38.02%  Val Acc: 51.94%\n",
      "Saved baseline clean state_dict -> baseline_vgg11.pth\n",
      "✓ Saved new best model (51.94%)\n",
      "[Epoch 2/15] Train Acc: 53.41%  Val Acc: 60.48%\n",
      "Saved baseline clean state_dict -> baseline_vgg11.pth\n",
      "✓ Saved new best model (60.48%)\n",
      "[Epoch 3/15] Train Acc: 59.80%  Val Acc: 65.38%\n",
      "Saved baseline clean state_dict -> baseline_vgg11.pth\n",
      "✓ Saved new best model (65.38%)\n",
      "[Epoch 4/15] Train Acc: 64.26%  Val Acc: 69.24%\n",
      "Saved baseline clean state_dict -> baseline_vgg11.pth\n",
      "✓ Saved new best model (69.24%)\n",
      "[Epoch 5/15] Train Acc: 67.56%  Val Acc: 70.26%\n",
      "Saved baseline clean state_dict -> baseline_vgg11.pth\n",
      "✓ Saved new best model (70.26%)\n",
      "[Epoch 6/15] Train Acc: 70.17%  Val Acc: 72.88%\n",
      "Saved baseline clean state_dict -> baseline_vgg11.pth\n",
      "✓ Saved new best model (72.88%)\n",
      "[Epoch 7/15] Train Acc: 72.04%  Val Acc: 71.44%\n",
      "[Epoch 8/15] Train Acc: 73.58%  Val Acc: 74.86%\n",
      "Saved baseline clean state_dict -> baseline_vgg11.pth\n",
      "✓ Saved new best model (74.86%)\n",
      "[Epoch 9/15] Train Acc: 74.99%  Val Acc: 75.90%\n",
      "Saved baseline clean state_dict -> baseline_vgg11.pth\n",
      "✓ Saved new best model (75.90%)\n",
      "[Epoch 10/15] Train Acc: 76.66%  Val Acc: 77.70%\n",
      "Saved baseline clean state_dict -> baseline_vgg11.pth\n",
      "✓ Saved new best model (77.70%)\n",
      "[Epoch 11/15] Train Acc: 77.35%  Val Acc: 77.48%\n",
      "[Epoch 12/15] Train Acc: 78.52%  Val Acc: 78.20%\n",
      "Saved baseline clean state_dict -> baseline_vgg11.pth\n",
      "✓ Saved new best model (78.20%)\n",
      "[Epoch 13/15] Train Acc: 79.11%  Val Acc: 79.28%\n",
      "Saved baseline clean state_dict -> baseline_vgg11.pth\n",
      "✓ Saved new best model (79.28%)\n",
      "[Epoch 14/15] Train Acc: 79.95%  Val Acc: 78.96%\n",
      "[Epoch 15/15] Train Acc: 80.79%  Val Acc: 79.62%\n",
      "Saved baseline clean state_dict -> baseline_vgg11.pth\n",
      "✓ Saved new best model (79.62%)\n",
      "Baseline training complete. Best validation accuracy: 79.62%\n"
     ]
    }
   ],
   "source": [
    "baseline_ckpt_path = \"baseline_vgg11.pth\"   \n",
    "\n",
    "train_baseline( baseline_path=baseline_ckpt_path, epochs=15, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ePmXdII4iywj",
    "outputId": "651e407d-1d24-4aa0-c384-d2bc6b79ba30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sensitivity analysis (unstructured). This may take some time...\n",
      "Baseline accuracy (loaded): 80.19\n",
      "[sens] features.0.weight @ 50% -> no_ft 74.16\n",
      "[sens] features.0.weight @ 70% -> no_ft 49.70\n",
      "[sens] features.4.weight @ 50% -> no_ft 79.27\n",
      "[sens] features.4.weight @ 70% -> no_ft 71.81\n",
      "[sens] features.8.weight @ 50% -> no_ft 78.64\n",
      "[sens] features.8.weight @ 70% -> no_ft 69.29\n",
      "[sens] features.11.weight @ 50% -> no_ft 78.66\n",
      "[sens] features.11.weight @ 70% -> no_ft 73.23\n",
      "[sens] features.15.weight @ 50% -> no_ft 78.98\n",
      "[sens] features.15.weight @ 70% -> no_ft 75.73\n",
      "[sens] features.18.weight @ 50% -> no_ft 79.57\n",
      "[sens] features.18.weight @ 70% -> no_ft 77.63\n",
      "[sens] features.22.weight @ 50% -> no_ft 79.63\n",
      "[sens] features.22.weight @ 70% -> no_ft 77.99\n",
      "[sens] features.25.weight @ 50% -> no_ft 80.23\n",
      "[sens] features.25.weight @ 70% -> no_ft 79.97\n",
      "[sens] classifier.0.weight @ 50% -> no_ft 80.10\n",
      "[sens] classifier.0.weight @ 70% -> no_ft 80.20\n",
      "[sens] classifier.3.weight @ 50% -> no_ft 80.18\n",
      "[sens] classifier.3.weight @ 70% -> no_ft 80.31\n",
      "[sens] classifier.6.weight @ 50% -> no_ft 80.11\n",
      "[sens] classifier.6.weight @ 70% -> no_ft 80.03\n",
      "Saved sensitivity results to unstructured_outputs/sensitivity\n",
      "Saved layer ranking (least sensitive first) → unstructured_outputs/sensitivity/ranked_layers_by_sensitivity.json\n",
      "Sensitivity done. Baseline accuracy: 80.19\n",
      "[allocator] total_params=128796352, target_prune_params=90157446, estimated_pruned=90157446, est_global_sparsity=0.7000\n",
      "Saved suggested per-layer sparsities to unstructured_outputs/suggested_unstruct_sparsities.json\n",
      "Initial accuracy after applying sparsities: 21.54%\n",
      "[Finetune 1/10] Train Acc=68.19%, Val Acc=71.80%\n",
      "[Finetune 2/10] Train Acc=74.65%, Val Acc=75.73%\n",
      "[Finetune 3/10] Train Acc=77.62%, Val Acc=78.24%\n",
      "[Finetune 4/10] Train Acc=79.41%, Val Acc=77.16%\n",
      "[Finetune 5/10] Train Acc=80.87%, Val Acc=78.83%\n",
      "[Finetune 6/10] Train Acc=82.19%, Val Acc=81.28%\n",
      "[Finetune 7/10] Train Acc=83.15%, Val Acc=83.21%\n",
      "[Finetune 8/10] Train Acc=84.36%, Val Acc=81.67%\n",
      "[Finetune 9/10] Train Acc=84.89%, Val Acc=83.28%\n",
      "[Finetune 10/10] Train Acc=85.70%, Val Acc=83.87%\n",
      "Final global sparsity: 70.00%\n",
      "Saved final unstructured state_dict: unstructured_outputs/final/final_pruned_state_dict.pth\n",
      "Final unstructured model saved. Global sparsity: 0.699999996894322\n"
     ]
    }
   ],
   "source": [
    "baseline_ckpt_path = \"baseline_vgg11.pth\"   \n",
    "\n",
    "results = run_unstructured_pipeline( baseline_checkpoint=baseline_ckpt_path, sparsity_list_percent=[50, 70], short_finetune_epochs=0, final_finetune_epochs=10, out_dir='unstructured_outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HO0XZYdsO4UF",
    "outputId": "329c28b1-9428-46b8-d16f-78c0c4119d63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of pruned model: 83.87%\n"
     ]
    }
   ],
   "source": [
    "pruned_model = results[0]\n",
    "print(f\"Test Accuracy of pruned model: {evaluate(pruned_model, CIFAR10_testloader)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T18:59:40.259691Z",
     "iopub.status.busy": "2025-11-01T18:59:40.259368Z",
     "iopub.status.idle": "2025-11-01T18:59:43.861623Z",
     "shell.execute_reply": "2025-11-01T18:59:43.860824Z",
     "shell.execute_reply.started": "2025-11-01T18:59:40.259668Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: torch.load raised an unpickling/weights-only error. Retrying with a safe-globals allowlist (only do this for trusted checkpoints).\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "def load_unstructured_checkpoint(path, device=None):\n",
    "    if device is None:\n",
    "        device = globals().get('device', 'cpu')\n",
    "    device = torch.device(device)\n",
    "    ck = torch.load(path, map_location='cpu')  \n",
    "    state = ck.get('model_state_dict', ck)\n",
    "    num_classes = ck.get('num_classes', None)\n",
    "\n",
    "    model = torchvision.models.vgg11_bn(pretrained=False)\n",
    "    if num_classes is not None and model.classifier[6].out_features != int(num_classes):\n",
    "        model.classifier[6] = torch.nn.Linear(model.classifier[6].in_features, int(num_classes))\n",
    "\n",
    "    new_state = {}\n",
    "    for k, v in state.items():\n",
    "        new_k = k[len('module.'):] if k.startswith('module.') else k\n",
    "        new_state[new_k] = v\n",
    "\n",
    "    try:\n",
    "        model.load_state_dict(new_state, strict=True)\n",
    "    except RuntimeError as e:\n",
    "        print(\"Strict load failed for unstructured checkpoint:\", e)\n",
    "        model.load_state_dict(new_state, strict=False)\n",
    "        print(\"Loaded with strict=False (check missing/unexpected keys).\")\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model, ck\n",
    "\n",
    "def load_structured_checkpoint(path, device=None):\n",
    "    if device is None:\n",
    "        device = globals().get('device', 'cpu')\n",
    "    device = torch.device(device)\n",
    "    ck = torch.load(path, map_location='cpu')\n",
    "    state = ck.get('model_state_dict', ck)\n",
    "    kept_indices = ck.get('kept_indices', None)\n",
    "    if kept_indices is None:\n",
    "        raise RuntimeError(\"structured checkpoint missing 'kept_indices' — cannot rebuild pruned architecture.\")\n",
    "\n",
    "    normalized = {}\n",
    "    for k, v in kept_indices.items():\n",
    "        key = k\n",
    "        if key.endswith('.weight') or key.endswith('.bias'):\n",
    "            key = '.'.join(key.split('.')[:2])\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            lst = v.cpu().tolist()\n",
    "        elif isinstance(v, (list, tuple)):\n",
    "            lst = [int(x) for x in v]\n",
    "        elif isinstance(v, np.ndarray):\n",
    "            lst = v.tolist()\n",
    "\n",
    "def _resolve_kaggle_path(path: str) -> str:\n",
    "    if os.path.exists(path):\n",
    "        return path\n",
    "    base = '/kaggle/input'\n",
    "    if not os.path.exists(base):\n",
    "        return path\n",
    "    candidate = os.path.join(base, path)\n",
    "    if os.path.exists(candidate):\n",
    "        return candidate\n",
    "    filename = os.path.basename(path)\n",
    "    for root, _, files in os.walk(base):\n",
    "        if filename in files:\n",
    "            return os.path.join(root, filename)\n",
    "    return path\n",
    "\n",
    "def _torch_load_robust(path: str):\n",
    "    try:\n",
    "        return torch.load(path, map_location='cpu')\n",
    "    except Exception as e:\n",
    "        msg = str(e)\n",
    "        if 'WeightsUnpickler' in msg or 'multiarray._reconstruct' in msg or 'UnpicklingError' in msg:\n",
    "            print(\"WARN: torch.load raised an unpickling/weights-only error. Retrying with a safe-globals allowlist (only do this for trusted checkpoints).\")\n",
    "            try:\n",
    "                add_safe_globals([np.core.multiarray._reconstruct])\n",
    "            except Exception:\n",
    "                try:\n",
    "                    add_safe_globals([np._core.multiarray._reconstruct])\n",
    "                except Exception:\n",
    "                    pass\n",
    "            return torch.load(path, map_location='cpu', weights_only=False)\n",
    "        raise\n",
    "\n",
    "def load_unstructured_checkpoint(path: str, device=None) -> Tuple[torch.nn.Module, Dict[str,Any]]:\n",
    "    resolved = _resolve_kaggle_path(path)\n",
    "    if not os.path.exists(resolved):\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {resolved}\")\n",
    "    ck = _torch_load_robust(resolved)\n",
    "    state = ck.get('model_state_dict', ck) if isinstance(ck, dict) else ck\n",
    "    num_classes = ck.get('num_classes', None) if isinstance(ck, dict) else None\n",
    "\n",
    "    model = torchvision.models.vgg11_bn(pretrained=False)\n",
    "    if num_classes is not None:\n",
    "        numc = int(num_classes)\n",
    "        if model.classifier[6].out_features != numc:\n",
    "            model.classifier[6] = nn.Linear(model.classifier[6].in_features, numc)\n",
    "\n",
    "    new_state = {}\n",
    "    for k, v in state.items():\n",
    "        new_k = k[len('module.'): ] if k.startswith('module.') else k\n",
    "        new_state[new_k] = v\n",
    "\n",
    "    try:\n",
    "        model.load_state_dict(new_state, strict=True)\n",
    "    except Exception as e:\n",
    "        print(\"Strict load failed for unstructured checkpoint:\", e)\n",
    "        res = model.load_state_dict(new_state, strict=False)\n",
    "        print(\"Loaded with strict=False. Missing keys:\", res.missing_keys, \"Unexpected keys:\", res.unexpected_keys)\n",
    "\n",
    "    if device is None:\n",
    "        device = globals().get('device', 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(torch.device(device))\n",
    "    model.eval()\n",
    "    return model, ck\n",
    "\n",
    "\n",
    "def load_structured_checkpoint(path: str, device=None) -> Tuple[torch.nn.Module, Dict[str, list], Dict[str,Any]]:\n",
    "    resolved = _resolve_kaggle_path(path)\n",
    "    if not os.path.exists(resolved):\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {resolved}\")\n",
    "    ck = _torch_load_robust(resolved)\n",
    "    state = ck.get('model_state_dict', ck) if isinstance(ck, dict) else ck\n",
    "    kept_indices = ck.get('kept_indices', None) if isinstance(ck, dict) else None\n",
    "    if kept_indices is None:\n",
    "        raise RuntimeError(\"Structured checkpoint missing 'kept_indices' — cannot rebuild pruned architecture.\")\n",
    "\n",
    "    normalized = {}\n",
    "    for k, v in kept_indices.items():\n",
    "        key = k\n",
    "        if key.endswith('.weight') or key.endswith('.bias') or key.endswith('.running_mean') or key.endswith('.running_var'):\n",
    "            key = '.'.join(key.split('.')[:2])\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            lst = v.cpu().tolist()\n",
    "        elif isinstance(v, np.ndarray):\n",
    "            lst = v.tolist()\n",
    "        elif isinstance(v, (list, tuple)):\n",
    "            lst = [int(x) for x in v]\n",
    "        else:\n",
    "            lst = [int(v)]\n",
    "        normalized[key] = lst\n",
    "\n",
    "    pruned_model, used_kept = build_pruned_vgg(normalized)\n",
    "    pruned_model.to('cpu') \n",
    "    \n",
    "    new_state = {}\n",
    "    for k, v in state.items():\n",
    "        new_k = k[len('module.'):] if k.startswith('module.') else k\n",
    "        new_state[new_k] = v\n",
    "\n",
    "    try:\n",
    "        pruned_model.load_state_dict(new_state, strict=True)\n",
    "    except Exception as e:\n",
    "        print(\"Strict load failed for structured checkpoint (expected if shapes/buffers differ):\", e)\n",
    "        res = pruned_model.load_state_dict(new_state, strict=False)\n",
    "        print(\"Loaded with strict=False. Missing keys:\", res.missing_keys, \"Unexpected keys:\", res.unexpected_keys)\n",
    "\n",
    "    if device is None:\n",
    "        device = globals().get('device', 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    pruned_model.to(torch.device(device))\n",
    "    pruned_model.eval()\n",
    "    return pruned_model, normalized, ck\n",
    "\n",
    "model_u, ck_u = load_unstructured_checkpoint('/kaggle/input/unstructured/final/final_pruned_state_dict.pth', device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T19:20:38.848789Z",
     "iopub.status.busy": "2025-11-01T19:20:38.847871Z",
     "iopub.status.idle": "2025-11-01T19:20:43.556291Z",
     "shell.execute_reply": "2025-11-01T19:20:43.555644Z",
     "shell.execute_reply.started": "2025-11-01T19:20:38.848761Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: total weight elements = 128,799,104 > 5,000,000; sampling for plotting.\n",
      "WARN: torch.load raised an unpickling/weights-only error. Retrying with a safe-globals allowlist.\n",
      "Note: total weight elements = 128,799,104 > 5,000,000; sampling for plotting.\n",
      "Saved weight distribution comparison to weight_dists.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def _torch_load_robust(path):\n",
    "    try:\n",
    "        return torch.load(path, map_location='cpu')\n",
    "    except Exception as e:\n",
    "        msg = str(e)\n",
    "        if 'WeightsUnpickler' in msg or 'multiarray._reconstruct' in msg or 'UnpicklingError' in msg:\n",
    "            print(\"WARN: torch.load raised an unpickling/weights-only error. Retrying with a safe-globals allowlist.\")\n",
    "            try:\n",
    "                add_safe_globals([np.core.multiarray._reconstruct])\n",
    "            except Exception:\n",
    "                try:\n",
    "                    add_safe_globals([np._core.multiarray._reconstruct])\n",
    "                except Exception:\n",
    "                    pass\n",
    "            return torch.load(path, map_location='cpu', weights_only=False)\n",
    "        raise\n",
    "\n",
    "def _extract_weight_values_from_state_dict(sd, max_total_elems=5_000_000):\n",
    "    vals_list = []\n",
    "    total_elems = 0\n",
    "    for k, v in sd.items():\n",
    "        if 'weight' not in k:\n",
    "            continue\n",
    "        try:\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                arr = v.detach().cpu().numpy()\n",
    "            elif isinstance(v, np.ndarray):\n",
    "                arr = v\n",
    "            elif isinstance(v, (list, tuple)):\n",
    "                arr = np.asarray(v)\n",
    "            else:\n",
    "                continue\n",
    "        except Exception:\n",
    "            continue\n",
    "        if arr.size == 0:\n",
    "            continue\n",
    "        vals_list.append(arr.ravel())\n",
    "        total_elems += arr.size\n",
    "\n",
    "    if len(vals_list) == 0:\n",
    "        return np.array([], dtype=np.float32)\n",
    "\n",
    "    if total_elems <= max_total_elems:\n",
    "        return np.concatenate(vals_list).astype(np.float32)\n",
    "\n",
    "    print(f\"Note: total weight elements = {total_elems:,} > {max_total_elems:,}; sampling for plotting.\")\n",
    "    concat_len = total_elems\n",
    "    rng = np.random.default_rng(42)\n",
    "    idx = rng.choice(concat_len, size=max_total_elems, replace=False)\n",
    "    idx.sort()\n",
    "    out = np.empty(len(idx), dtype=np.float32)\n",
    "    pos = 0\n",
    "    sample_ptr = 0\n",
    "    for arr in vals_list:\n",
    "        n = arr.size\n",
    "        lo = np.searchsorted(idx, pos, side='left')\n",
    "        hi = np.searchsorted(idx, pos+n, side='left')\n",
    "        if lo < hi:\n",
    "            relative = idx[lo:hi] - pos\n",
    "            out[sample_ptr:sample_ptr + (hi-lo)] = arr.ravel()[relative]\n",
    "            sample_ptr += (hi-lo)\n",
    "        pos += n\n",
    "    return out[:sample_ptr].astype(np.float32)\n",
    "\n",
    "def plot_weight_distributions(baseline_ckpt, pruned_ckpt, out_path='weight_dists.png', bins=200, log_scale=False, per_layer=False, sample_limit=5_000_000):\n",
    "   \n",
    "    for p in (baseline_ckpt, pruned_ckpt):\n",
    "        if not os.path.exists(p):\n",
    "            raise FileNotFoundError(f\"Checkpoint file not found: {p}\")\n",
    "\n",
    "    def load_and_extract(p):\n",
    "        ck = _torch_load_robust(p)\n",
    "        state = ck.get('model_state_dict', ck) if isinstance(ck, dict) else ck\n",
    "        # ensure it's a mapping\n",
    "        if not isinstance(state, dict):\n",
    "            raise RuntimeError(\"Loaded checkpoint did not contain a state-dict mapping.\")\n",
    "        vals = _extract_weight_values_from_state_dict(state, max_total_elems=sample_limit)\n",
    "        return vals, state\n",
    "\n",
    "    w_base, sd_base = load_and_extract(baseline_ckpt)\n",
    "    w_pruned, sd_pruned = load_and_extract(pruned_ckpt)\n",
    "\n",
    "    if w_base.size == 0:\n",
    "        raise RuntimeError(\"Baseline checkpoint contains no weight tensors matching 'weight' keys.\")\n",
    "    if w_pruned.size == 0:\n",
    "        print(\"Warning: pruned checkpoint contains no weight tensors matching 'weight' keys — plot will be empty for pruned.\")\n",
    "\n",
    "    abs_base = np.abs(w_base)\n",
    "    abs_pruned = np.abs(w_pruned)\n",
    "\n",
    "    combined = np.concatenate([abs_base, abs_pruned]) if abs_pruned.size>0 else abs_base\n",
    "    if combined.size == 0:\n",
    "        raise RuntimeError(\"No numeric weight data found in either checkpoint.\")\n",
    "    eps = 1e-12\n",
    "    hist_min, hist_max = combined.min(), combined.max() + eps\n",
    "    if hist_min == hist_max:\n",
    "        hist_max = hist_min + 1e-6\n",
    "    bin_edges = np.linspace(hist_min, hist_max, bins+1)\n",
    "\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.hist(abs_base, bins=bin_edges, alpha=0.7, label='Before pruning', density=False)\n",
    "    if abs_pruned.size > 0:\n",
    "        plt.hist(abs_pruned, bins=bin_edges, alpha=0.7, label='After pruning', density=False)\n",
    "    plt.xlabel('|Weight|')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Distribution of Weight Magnitudes (Before vs After Pruning)')\n",
    "    plt.legend()\n",
    "    if log_scale:\n",
    "        plt.yscale('log')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path)\n",
    "    plt.close()\n",
    "    print(\"Saved weight distribution comparison to\", out_path)\n",
    "\n",
    "    if per_layer:\n",
    "        base_dir = os.path.splitext(out_path)[0] + \"_per_layer\"\n",
    "        os.makedirs(base_dir, exist_ok=True)\n",
    "        def safe_to_array(v):\n",
    "            try:\n",
    "                if isinstance(v, torch.Tensor): return v.detach().cpu().numpy().ravel()\n",
    "                if isinstance(v, np.ndarray): return v.ravel()\n",
    "                if isinstance(v, (list,tuple)): return np.asarray(v).ravel()\n",
    "            except Exception:\n",
    "                return np.array([], dtype=np.float32)\n",
    "        keys = sorted([k for k in sd_base.keys() if 'weight' in k])\n",
    "        for k in keys:\n",
    "            a = safe_to_array(sd_base.get(k, np.array([])))\n",
    "            b = safe_to_array(sd_pruned.get(k, np.array([])))\n",
    "            if a.size == 0 and b.size == 0:\n",
    "                continue\n",
    "            if a.size > sample_limit:\n",
    "                rng = np.random.default_rng(0); a = rng.choice(a, sample_limit, replace=False)\n",
    "            if b.size > sample_limit:\n",
    "                rng = np.random.default_rng(0); b = rng.choice(b, sample_limit, replace=False)\n",
    "            plt.figure(figsize=(6,3))\n",
    "            plt.hist(np.abs(a), bins=50, alpha=0.6, label='before')\n",
    "            if b.size>0:\n",
    "                plt.hist(np.abs(b), bins=50, alpha=0.6, label='after')\n",
    "            plt.title(k); plt.xlabel('|w|'); plt.ylabel('count'); plt.legend()\n",
    "            plt.tight_layout()\n",
    "            outk = os.path.join(base_dir, k.replace('.','_') + '.png')\n",
    "            plt.savefig(outk); plt.close()\n",
    "        print(f\"Saved per-layer plots to {base_dir}\")\n",
    "\n",
    "plot_weight_distributions(\"baseline_vgg11.pth\",\n",
    "                         \"/kaggle/input/unstructured/final/final_pruned_state_dict.pth\",  out_path='weight_dists.png', bins=200, log_scale=True, per_layer=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T21:26:28.987886Z",
     "iopub.status.busy": "2025-11-01T21:26:28.987127Z",
     "iopub.status.idle": "2025-11-01T21:26:29.161928Z",
     "shell.execute_reply": "2025-11-01T21:26:29.161288Z",
     "shell.execute_reply.started": "2025-11-01T21:26:28.987857Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ranked plot to sensitivity_vis/sensitivity_ranked_from_json.png\n",
      "Top 5 most sensitive layers:\n",
      "  features.0.weight               drop=18.26\n",
      "  features.8.weight               drop=6.22\n",
      "  features.4.weight               drop=4.65\n",
      "  features.11.weight              drop=4.25\n",
      "  features.15.weight              drop=2.83\n",
      "Top 5 least sensitive layers:\n",
      "  features.22.weight              drop=1.38\n",
      "  classifier.6.weight             drop=0.12\n",
      "  features.25.weight              drop=0.09\n",
      "  classifier.0.weight             drop=0.04\n",
      "  classifier.3.weight             drop=-0.06\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ranked_path = \"/kaggle/input/unstructured/sensitivity/ranked_layers_by_sensitivity.json\"  # adjust path\n",
    "out_dir = \"sensitivity_vis\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "with open(ranked_path, \"r\") as f:\n",
    "    ranked_layers = json.load(f)\n",
    "\n",
    "if isinstance(ranked_layers, dict):\n",
    "    items = list(ranked_layers.items())\n",
    "elif isinstance(ranked_layers, list):\n",
    "    items = [(k, float(v)) for k, v in ranked_layers]\n",
    "else:\n",
    "    raise RuntimeError(f\"Unexpected type: {type(ranked_layers)}\")\n",
    "\n",
    "items = sorted(items, key=lambda x: x[1], reverse=True) \n",
    "layers = [k for k, _ in items]\n",
    "drops  = [v for _, v in items]\n",
    "\n",
    "topN = min(40, len(layers))\n",
    "plt.figure(figsize=(8, max(3, 0.25 * topN)))\n",
    "plt.barh(np.arange(topN)[::-1], drops[:topN])\n",
    "plt.yticks(np.arange(topN), layers[:topN][::-1])\n",
    "plt.xlabel(\"Average accuracy drop (%)\")\n",
    "plt.title(\"Layer sensitivity ranking (higher = more sensitive)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(out_dir, \"sensitivity_ranked_from_json.png\"))\n",
    "plt.close()\n",
    "print(f\"Saved ranked plot to {os.path.join(out_dir, 'sensitivity_ranked_from_json.png')}\")\n",
    "\n",
    "print(\"Top 5 most sensitive layers:\")\n",
    "for k, v in items[:5]:\n",
    "    print(f\"  {k:30s}  drop={v:.2f}\")\n",
    "print(\"Top 5 least sensitive layers:\")\n",
    "for k, v in items[-5:]:\n",
    "    print(f\"  {k:30s}  drop={v:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQ7U8qcM16R8"
   },
   "source": [
    "*Part 2*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T19:45:55.636231Z",
     "iopub.status.busy": "2025-11-01T19:45:55.635544Z",
     "iopub.status.idle": "2025-11-01T19:45:55.641864Z",
     "shell.execute_reply": "2025-11-01T19:45:55.641072Z",
     "shell.execute_reply.started": "2025-11-01T19:45:55.636206Z"
    },
    "id": "t9oIvK5G14Se",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def collect_convs_from_global_vgg():\n",
    "  convs = []\n",
    "  for idx, mod in enumerate(vgg_model.features):\n",
    "      if isinstance(mod, nn.Conv2d):\n",
    "          convs.append((f\"features.{idx}\", mod))\n",
    "  return convs\n",
    "\n",
    "def channel_l2_norms(conv_mod):\n",
    "  w = conv_mod.weight.data.detach()\n",
    "  norms = torch.norm(w.view(w.shape[0], -1), p=2, dim=1)  \n",
    "  return norms\n",
    "\n",
    "def choose_kept_channels(conv_mod, keep_fraction):\n",
    "  norms = channel_l2_norms(conv_mod).cpu()\n",
    "  k = max(1, int(round(keep_fraction * norms.numel())))\n",
    "  vals, idx = torch.topk(norms, k=k, largest=True)\n",
    "  kept = sorted(idx.tolist())\n",
    "  return kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T19:46:04.512549Z",
     "iopub.status.busy": "2025-11-01T19:46:04.512005Z",
     "iopub.status.idle": "2025-11-01T19:46:04.534893Z",
     "shell.execute_reply": "2025-11-01T19:46:04.534278Z",
     "shell.execute_reply.started": "2025-11-01T19:46:04.512524Z"
    },
    "id": "QY2FSrGL2FEy",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def build_pruned_vgg(kept_map):\n",
    "    orig = vgg_model  \n",
    "    orig.eval()\n",
    "    conv_entries = collect_convs_from_global_vgg()\n",
    "    conv_names = [n for n,_ in conv_entries]\n",
    "\n",
    "    kept = {}\n",
    "    for name, conv in conv_entries:\n",
    "        if name in kept_map:\n",
    "            kept[name] = list(kept_map[name])\n",
    "        else:\n",
    "            kept[name] = list(range(conv.out_channels))\n",
    "\n",
    "    new_features = []\n",
    "    prev_kept = None\n",
    "    conv_iter = iter(conv_entries)\n",
    "    for idx, module in enumerate(orig.features):\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            name, old_conv = next(conv_iter)\n",
    "            kept_out = kept[name]\n",
    "            if prev_kept is None:\n",
    "                kept_in = list(range(old_conv.in_channels))\n",
    "            else:\n",
    "                kept_in = prev_kept\n",
    "            new_conv = nn.Conv2d(in_channels=len(kept_in),\n",
    "                                 out_channels=len(kept_out),\n",
    "                                 kernel_size=old_conv.kernel_size,\n",
    "                                 stride=old_conv.stride,\n",
    "                                 padding=old_conv.padding,\n",
    "                                 dilation=old_conv.dilation,\n",
    "                                 groups=old_conv.groups,\n",
    "                                 bias=(old_conv.bias is not None))\n",
    "            with torch.no_grad():\n",
    "                old_w = old_conv.weight.data.cpu()\n",
    "                new_w = old_w[kept_out, :, :, :].clone()\n",
    "                new_w = new_w[:, kept_in, :, :].clone()\n",
    "                new_conv.weight.data.copy_(new_w.to(new_conv.weight.device))\n",
    "                if old_conv.bias is not None:\n",
    "                    new_conv.bias.data.copy_(old_conv.bias.data.cpu()[kept_out].to(new_conv.bias.device))\n",
    "            new_features.append(new_conv)\n",
    "            prev_kept = kept_out\n",
    "        elif isinstance(module, nn.BatchNorm2d):\n",
    "            if prev_kept is None:\n",
    "                new_features.append(copy.deepcopy(module))\n",
    "            else:\n",
    "                kept_out = prev_kept\n",
    "                new_bn = nn.BatchNorm2d(num_features=len(kept_out))\n",
    "                with torch.no_grad():\n",
    "                    new_bn.weight.data.copy_(module.weight.data.cpu()[kept_out].to(new_bn.weight.device))\n",
    "                    new_bn.bias.data.copy_(module.bias.data.cpu()[kept_out].to(new_bn.bias.device))\n",
    "                    new_bn.running_mean.data.copy_(module.running_mean.data.cpu()[kept_out].to(new_bn.running_mean.device))\n",
    "                    new_bn.running_var.data.copy_(module.running_var.data.cpu()[kept_out].to(new_bn.running_var.device))\n",
    "                new_features.append(new_bn)\n",
    "        else:\n",
    "            new_features.append(copy.deepcopy(module))\n",
    "\n",
    "    new_features = nn.Sequential(*new_features)\n",
    "\n",
    "    new_vgg = torchvision.models.vgg11_bn(pretrained=False)\n",
    "    new_vgg.classifier[6] = nn.Linear(in_features=new_vgg.classifier[6].in_features,\n",
    "                                      out_features=vgg_model.classifier[6].out_features)\n",
    "    new_vgg.features = new_features\n",
    "\n",
    "    new_vgg.eval()\n",
    "    with torch.no_grad():\n",
    "        dummy = torch.randn(1,3,32,32).to(next(new_vgg.parameters()).device)\n",
    "        feat_out = new_vgg.features(dummy)\n",
    "        new_flat = int(feat_out.view(1,-1).shape[1])\n",
    "\n",
    "    old_clf = vgg_model.classifier\n",
    "    new_clf = []\n",
    "    for i, m in enumerate(old_clf):\n",
    "        if i == 0 and isinstance(m, nn.Linear):\n",
    "            new_m = nn.Linear(in_features=new_flat, out_features=m.out_features, bias=(m.bias is not None))\n",
    "            new_clf.append(new_m)\n",
    "        else:\n",
    "            new_clf.append(copy.deepcopy(m))\n",
    "    new_vgg.classifier = nn.Sequential(*new_clf)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        orig_for_fc = None\n",
    "        try:\n",
    "            baseline_ckpt = globals().get('baseline_ckpt_path', None)\n",
    "            if baseline_ckpt and os.path.exists(baseline_ckpt):\n",
    "                ck = torch.load(baseline_ckpt, map_location='cpu')\n",
    "                orig_for_fc = torchvision.models.vgg11_bn(pretrained=False)\n",
    "                orig_for_fc.classifier[6] = nn.Linear(orig_for_fc.classifier[6].in_features,\n",
    "                                                      vgg_model.classifier[6].out_features)\n",
    "                try:\n",
    "                    orig_for_fc.load_state_dict(ck['model_state_dict'])\n",
    "                except Exception:\n",
    "                    orig_for_fc.load_state_dict(ck, strict=False)\n",
    "                orig_for_fc.to(next(orig.parameters()).device)\n",
    "            else:\n",
    "                orig_for_fc = vgg_model\n",
    "        except Exception:\n",
    "            orig_for_fc = vgg_model\n",
    "\n",
    "        dummy2 = torch.randn(1,3,32,32).to(next(orig_for_fc.parameters()).device)\n",
    "        orig_feat = orig_for_fc.features(dummy2)\n",
    "        orig_C = orig_feat.shape[1]\n",
    "        H = orig_feat.shape[2]\n",
    "        W = orig_feat.shape[3]\n",
    "\n",
    "        last_conv_name = conv_names[-1]\n",
    "        last_kept = kept[last_conv_name]\n",
    "\n",
    "        old_fc0 = vgg_model.classifier[0]\n",
    "        old_w0 = old_fc0.weight.data.cpu()  \n",
    "        old_flat = old_fc0.in_features\n",
    "\n",
    "        if orig_C * H * W != old_flat:\n",
    "            spatial = old_flat // orig_C if orig_C > 0 else 1\n",
    "            h_w = int(math.sqrt(spatial))\n",
    "            if h_w * h_w == spatial:\n",
    "                H = W = h_w\n",
    "            else:\n",
    "                H = 1\n",
    "                W = spatial\n",
    "\n",
    "        block_size = H * W \n",
    "\n",
    "        keep_indices = []\n",
    "        for c in last_kept:\n",
    "            start = int(c) * block_size\n",
    "            keep_indices.extend(list(range(start, start + block_size)))\n",
    "        if len(keep_indices) == 0:\n",
    "            keep_indices = list(range(min(block_size, old_flat)))\n",
    "\n",
    "        keep_idx = torch.tensor(keep_indices, dtype=torch.long)\n",
    "\n",
    "        new_w0 = old_w0.index_select(dim=1, index=keep_idx)  \n",
    "\n",
    "        new_in = new_w0.shape[1]\n",
    "        new_out = old_w0.shape[0]\n",
    "        new_fc0 = nn.Linear(in_features=new_in, out_features=new_out, bias=(old_fc0.bias is not None))\n",
    "        new_fc0.weight.data.copy_(new_w0.to(new_fc0.weight.device))\n",
    "        new_fc0.bias.data.copy_(old_fc0.bias.data.clone().to(new_fc0.bias.device))\n",
    "        new_vgg.classifier[0] = new_fc0.to(next(new_vgg.parameters()).device)\n",
    "\n",
    "        for i in range(1, len(old_clf)):\n",
    "            if isinstance(old_clf[i], nn.Linear) and isinstance(new_vgg.classifier[i], nn.Linear):\n",
    "                ow = old_clf[i].weight.data.clone()\n",
    "                ob = old_clf[i].bias.data.clone()\n",
    "                if new_vgg.classifier[i].weight.data.shape == ow.shape:\n",
    "                    new_vgg.classifier[i].weight.data.copy_(ow.to(new_vgg.classifier[i].weight.device))\n",
    "                    new_vgg.classifier[i].bias.data.copy_(ob.to(new_vgg.classifier[i].bias.device))\n",
    "                else:\n",
    "                    min_r = min(ow.shape[0], new_vgg.classifier[i].weight.data.shape[0])\n",
    "                    min_c = min(ow.shape[1], new_vgg.classifier[i].weight.data.shape[1])\n",
    "                    if min_r > 0 and min_c > 0:\n",
    "                        new_vgg.classifier[i].weight.data[:min_r,:min_c].copy_(ow[:min_r,:min_c].to(new_vgg.classifier[i].weight.device))\n",
    "                        new_vgg.classifier[i].bias.data[:min_r].copy_(ob[:min_r].to(new_vgg.classifier[i].bias.device))\n",
    "\n",
    "    new_vgg.to(device)\n",
    "    return new_vgg, kept\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T19:46:09.105880Z",
     "iopub.status.busy": "2025-11-01T19:46:09.105613Z",
     "iopub.status.idle": "2025-11-01T19:46:09.116478Z",
     "shell.execute_reply": "2025-11-01T19:46:09.115735Z",
     "shell.execute_reply.started": "2025-11-01T19:46:09.105860Z"
    },
    "id": "lud-LhBt2G5B",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def structured_sensitivity(sparsity_list_percent=[50,70], out_dir='structured_sensitivity', eval_loader=None):\n",
    " \n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    convs = collect_convs_from_global_vgg()\n",
    "    results = {}\n",
    "\n",
    "    if eval_loader is None:\n",
    "        eval_loader = CIFAR10_testloader\n",
    "\n",
    "    baseline = vgg_model\n",
    "    baseline.to(device)\n",
    "    baseline.eval()\n",
    "    baseline_acc = evaluate(baseline, eval_loader)\n",
    "    print(f\"[structured_sensitivity] Baseline accuracy on eval_loader: {baseline_acc:.2f}%\")\n",
    "\n",
    "    for (layer_name, conv_mod) in convs:\n",
    "        results[layer_name] = {'sparsities': [], 'acc_no_ft': []}\n",
    "        norms_before = channel_l2_norms(conv_mod).cpu().numpy()\n",
    "        for s in sparsity_list_percent:\n",
    "            sp = float(s) / 100.0\n",
    "            keep_frac = max(0.0, 1.0 - sp)\n",
    "\n",
    "            kept_map = {}\n",
    "            for nm, cm in convs:\n",
    "                if nm == layer_name:\n",
    "                    kept_map[nm] = choose_kept_channels(cm, keep_frac)\n",
    "                else:\n",
    "                    kept_map[nm] = list(range(cm.out_channels))\n",
    "\n",
    "            pruned_net, kept_used = build_pruned_vgg(kept_map)\n",
    "            pruned_net.to(device)\n",
    "            pruned_net.eval()\n",
    "\n",
    "            acc_no = evaluate(pruned_net, eval_loader)\n",
    "            results[layer_name]['sparsities'].append(s)\n",
    "            results[layer_name]['acc_no_ft'].append(acc_no)\n",
    "            print(f\"[structured_sens] {layer_name} @ {s}% -> no_ft {acc_no:.2f}\")\n",
    "\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.plot(results[layer_name]['sparsities'], results[layer_name]['acc_no_ft'], marker='o', label='no finetune')\n",
    "        plt.xlabel('Sparsity (%)'); plt.ylabel('Validation Accuracy (%)'); plt.title(layer_name)\n",
    "        plt.legend(); plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(out_dir, f\"{layer_name.replace('.','_')}_structured_sensitivity.png\"))\n",
    "        plt.close()\n",
    "\n",
    "    np.save(os.path.join(out_dir, 'structured_sensitivity_results.npy'), results)\n",
    "    avg_noft = {n: float(np.mean(results[n]['acc_no_ft'])) for n in results}\n",
    "    ranked = sorted(avg_noft.items(), key=lambda x: x[1], reverse=True)\n",
    "    ranked_path = os.path.join(out_dir, 'ranked_layers_by_noft.json')\n",
    "    with open(ranked_path, 'w') as f:\n",
    "        json.dump(ranked, f, indent=2)\n",
    "    print(f\"Saved structured sensitivity results and ranking to {out_dir} (ranking -> {ranked_path})\")\n",
    "    return results, baseline_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-11-01T19:46:47.541706Z",
     "iopub.status.busy": "2025-11-01T19:46:47.541464Z",
     "iopub.status.idle": "2025-11-01T19:48:51.420360Z",
     "shell.execute_reply": "2025-11-01T19:48:51.419492Z",
     "shell.execute_reply.started": "2025-11-01T19:46:47.541691Z"
    },
    "id": "mgq5qhLKq5CW",
    "outputId": "196c9483-856a-439f-ec9b-bd0d70970efc",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[structured_sensitivity] Baseline accuracy on eval_loader: 79.62%\n",
      "[structured_sens] features.0 @ 50% -> no_ft 50.24\n",
      "[structured_sens] features.0 @ 70% -> no_ft 31.24\n",
      "[structured_sens] features.4 @ 50% -> no_ft 40.48\n",
      "[structured_sens] features.4 @ 70% -> no_ft 25.42\n",
      "[structured_sens] features.8 @ 50% -> no_ft 58.04\n",
      "[structured_sens] features.8 @ 70% -> no_ft 42.78\n",
      "[structured_sens] features.11 @ 50% -> no_ft 44.82\n",
      "[structured_sens] features.11 @ 70% -> no_ft 22.32\n",
      "[structured_sens] features.15 @ 50% -> no_ft 66.38\n",
      "[structured_sens] features.15 @ 70% -> no_ft 28.48\n",
      "[structured_sens] features.18 @ 50% -> no_ft 49.94\n",
      "[structured_sens] features.18 @ 70% -> no_ft 14.30\n",
      "[structured_sens] features.22 @ 50% -> no_ft 76.46\n",
      "[structured_sens] features.22 @ 70% -> no_ft 64.60\n",
      "[structured_sens] features.25 @ 50% -> no_ft 78.72\n",
      "[structured_sens] features.25 @ 70% -> no_ft 69.60\n",
      "Saved structured sensitivity results and ranking to structured_quick (ranking -> structured_quick/ranked_layers_by_noft.json)\n"
     ]
    }
   ],
   "source": [
    "vgg_model.to(device)\n",
    "vgg_model.eval()  \n",
    "\n",
    "struct_results_quick, baseline_acc = structured_sensitivity( sparsity_list_percent=[50,70], out_dir='structured_quick', eval_loader=CIFAR10_valloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cZmU6JMHusPa"
   },
   "outputs": [],
   "source": [
    "def structured_finetune_topk(ranked_json='structured_quick/ranked_layers_by_noft.json', top_k=8, sparsity_percent=70, short_ft_epochs=2, eval_loader=None, out_dir='structured_topk'):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    ranked = json.load(open(ranked_json))\n",
    "    top_layers = [r[0] for r in ranked[:top_k]]\n",
    "    print(\"Top-k least-sensitive layers:\", top_layers)\n",
    "\n",
    "    convs = collect_convs_from_global_vgg()\n",
    "    results = {}\n",
    "    if eval_loader is None:\n",
    "        eval_loader = CIFAR10_testloader\n",
    "\n",
    "    for name, conv in convs:\n",
    "        results[name] = {'sparsity': sparsity_percent, 'acc_no_ft': None, 'acc_after_ft': None}\n",
    "        keep_map = {}\n",
    "        for nm, cm in convs:\n",
    "            if nm == name:\n",
    "                keep_frac = max(0.0, 1.0 - float(sparsity_percent)/100.0)\n",
    "                keep_map[nm] = choose_kept_channels(cm, keep_frac)\n",
    "            else:\n",
    "                keep_map[nm] = list(range(cm.out_channels))\n",
    "        pruned_net, kept_used = build_pruned_vgg(keep_map)\n",
    "        pruned_net.to(device)\n",
    "        pruned_net.eval()\n",
    "\n",
    "        acc_no = evaluate(pruned_net, eval_loader)\n",
    "        results[name]['acc_no_ft'] = acc_no\n",
    "\n",
    "        if name in top_layers:\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.SGD(pruned_net.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "            best_val = acc_no\n",
    "            no_imp = 0\n",
    "            for ep in range(short_ft_epochs):\n",
    "                train_one_epoch(pruned_net, CIFAR10_trainloader, criterion, optimizer)\n",
    "                val = evaluate(pruned_net, eval_loader)\n",
    "                if val > best_val + 1e-6:\n",
    "                    best_val = val; no_imp = 0\n",
    "                else:\n",
    "                    no_imp += 1\n",
    "                if no_imp >= 1:\n",
    "                    break\n",
    "            results[name]['acc_after_ft'] = best_val\n",
    "            print(f\"[topk_ft] {name} @ {sparsity_percent}% -> no_ft {acc_no:.2f}, after_ft {best_val:.2f}\")\n",
    "        else:\n",
    "            results[name]['acc_after_ft'] = None\n",
    "            print(f\"[topk_skip] {name} @ {sparsity_percent}% -> no_ft {acc_no:.2f} (skipped ft)\")\n",
    "\n",
    "    np.save(os.path.join(out_dir, 'structured_topk_results.npy'), results)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "55mbOryquvMg",
    "outputId": "d5e14eab-80e0-44a0-d2b5-69d205585c73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-k least-sensitive layers: ['features.25', 'features.22', 'features.18', 'features.0', 'features.15', 'features.8', 'features.11', 'features.4']\n",
      "[topk_ft] features.0 @ 70% -> no_ft 39.40, after_ft 73.75\n",
      "[topk_ft] features.4 @ 70% -> no_ft 21.50, after_ft 73.30\n",
      "[topk_ft] features.8 @ 70% -> no_ft 18.85, after_ft 70.10\n",
      "[topk_ft] features.11 @ 70% -> no_ft 17.00, after_ft 73.55\n",
      "[topk_ft] features.15 @ 70% -> no_ft 25.95, after_ft 76.15\n",
      "[topk_ft] features.18 @ 70% -> no_ft 35.80, after_ft 72.60\n",
      "[topk_ft] features.22 @ 70% -> no_ft 49.40, after_ft 73.65\n",
      "[topk_ft] features.25 @ 70% -> no_ft 76.10, after_ft 76.10\n"
     ]
    }
   ],
   "source": [
    "topk_results = structured_finetune_topk(ranked_json='structured_quick/ranked_layers_by_noft.json', top_k=8, sparsity_percent=70, \n",
    "                                        short_ft_epochs=2, eval_loader=CIFAR10_valloader, out_dir='structured_topk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jdWWCXLw72jR"
   },
   "outputs": [],
   "source": [
    "\n",
    "def suggest_structured_allocation(conv_list=None, sensitivity_results=None, target_param_reduction=0.70, max_prune_per_layer=0.90, anchor_sparsity=50):\n",
    "\n",
    "    if conv_list is None:\n",
    "        conv_list = collect_convs_from_global_vgg()\n",
    "\n",
    "    def param_count(m): return sum(p.numel() for p in m.parameters())\n",
    "    orig_total_params = param_count(vgg_model)\n",
    "    target_prune_params = int(round(target_param_reduction * orig_total_params))\n",
    "\n",
    "    layers = []\n",
    "    for name, conv in conv_list:\n",
    "        oc = conv.out_channels\n",
    "        ic = conv.in_channels\n",
    "        k_h, k_w = conv.kernel_size if isinstance(conv.kernel_size, tuple) else (conv.kernel_size, conv.kernel_size)\n",
    "        cost_per_out = int(ic * k_h * k_w) \n",
    "        layers.append({\n",
    "            'name': name,\n",
    "            'module': conv,\n",
    "            'out_channels': oc,\n",
    "            'in_channels': ic,\n",
    "            'cost_per_out': cost_per_out\n",
    "        })\n",
    "\n",
    "    scores = {}\n",
    "    for L in layers:\n",
    "        nm = L['name']\n",
    "        score = None\n",
    "        if sensitivity_results and nm in sensitivity_results:\n",
    "            res = sensitivity_results[nm]\n",
    "            spars_list = res.get('sparsities', [])\n",
    "            accs = res.get('acc_after_ft', [])\n",
    "            if len(spars_list) and len(accs):\n",
    "                idx = min(range(len(spars_list)), key=lambda i: abs(spars_list[i] - anchor_sparsity))\n",
    "                score = float(accs[idx])\n",
    "        if score is None:\n",
    "            score = 50.0\n",
    "        scores[nm] = score\n",
    "\n",
    "    layers_sorted = sorted(layers, key=lambda x: scores.get(x['name'], 0.0), reverse=True)\n",
    "\n",
    "    removed_params = 0\n",
    "    channels_to_remove = {L['name']: 0 for L in layers}\n",
    "    for L in layers_sorted:\n",
    "        name = L['name']\n",
    "        oc = L['out_channels']\n",
    "        cost = L['cost_per_out']\n",
    "        max_remove = int(math.floor(max_prune_per_layer * oc))\n",
    "        remaining = max(0, target_prune_params - removed_params)\n",
    "        if remaining <= 0:\n",
    "            break\n",
    "        need = cost and int(math.floor(remaining / cost)) or max_remove\n",
    "        remove_here = min(max_remove, need)\n",
    "        remove_here = min(remove_here, max(0, oc-1))\n",
    "        channels_to_remove[name] = remove_here\n",
    "        removed_params += remove_here * cost\n",
    "\n",
    "    if removed_params < target_prune_params:\n",
    "        candidates = []\n",
    "        for L in layers_sorted:\n",
    "            name = L['name']\n",
    "            oc = L['out_channels']\n",
    "            cost = L['cost_per_out']\n",
    "            max_remove = int(math.floor(max_prune_per_layer * oc))\n",
    "            remain_slots = max_remove - channels_to_remove[name]\n",
    "            for _ in range(remain_slots):\n",
    "                candidates.append((scores.get(name, 0.0), cost, name))\n",
    "        candidates.sort(key=lambda x: (x[0], x[1]), reverse=True)\n",
    "        idx = 0\n",
    "        while removed_params < target_prune_params and idx < len(candidates):\n",
    "            _, cost, name = candidates[idx]\n",
    "            Linfo = next(x for x in layers if x['name'] == name)\n",
    "            if channels_to_remove[name] < int(math.floor(max_prune_per_layer * Linfo['out_channels'])) and channels_to_remove[name] < Linfo['out_channels'] - 1:\n",
    "                channels_to_remove[name] += 1\n",
    "                removed_params += cost\n",
    "            idx += 1\n",
    "\n",
    "    per_layer_sparsities = {}\n",
    "    for L in layers:\n",
    "        name = L['name']\n",
    "        oc = L['out_channels']\n",
    "        rem = channels_to_remove.get(name, 0)\n",
    "        frac = float(rem) / float(oc)\n",
    "        frac = min(max(frac, 0.0), 0.95)\n",
    "        per_layer_sparsities[name] = frac\n",
    "\n",
    "    estimated_removed = sum(channels_to_remove[n] * next(l['cost_per_out'] for l in layers if l['name']==n) for n in channels_to_remove)\n",
    "    achieved_fraction = float(estimated_removed) / float(orig_total_params) if orig_total_params > 0 else 0.0\n",
    "\n",
    "    info = {\n",
    "        'orig_total_params': orig_total_params,\n",
    "        'target_prune_params': target_prune_params,\n",
    "        'estimated_removed_params': estimated_removed,\n",
    "        'estimated_fraction_reduction': achieved_fraction,\n",
    "        'channels_to_remove': channels_to_remove,\n",
    "        'sensitivity_scores': scores\n",
    "    }\n",
    "    return per_layer_sparsities, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PnFJs7nV74ru",
    "outputId": "c7a19bf9-d864-45e2-c04b-b9153df494be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated global reduction: 0.0642912688574995\n"
     ]
    }
   ],
   "source": [
    "conv_list = collect_convs_from_global_vgg()  \n",
    "per_layer_sparsities, info = suggest_structured_allocation(conv_list=conv_list,\n",
    "                                                           sensitivity_results=struct_results_quick,\n",
    "                                                           target_param_reduction=0.70,\n",
    "                                                           max_prune_per_layer=0.9,\n",
    "                                                           anchor_sparsity=50)\n",
    "print(\"Estimated global reduction:\", info['estimated_fraction_reduction'])\n",
    "import json\n",
    "with open('suggested_struct_sparsities.json','w') as f:\n",
    "    json.dump(per_layer_sparsities, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mJgSOxp-28Xt"
   },
   "outputs": [],
   "source": [
    "def apply_structured_and_finetune(per_layer_sparsities, finetune_epochs=20, out_dir='structured_final', \n",
    "                                  use_unstructured_ranked_path='unstructured_outputs/sensitivity/ranked_layers_by_sensitivity.json', \n",
    "                                  use_structured_ranked_path='structured_quick/ranked_layers_by_noft.json', ranked_k=8):\n",
    "\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    convs = collect_convs_from_global_vgg() \n",
    "    conv_names = [n for n,_ in convs]\n",
    "\n",
    "    candidate_layers = None\n",
    "\n",
    "    if use_structured_ranked_path and os.path.exists(use_structured_ranked_path):\n",
    "        try:\n",
    "            sr = json.load(open(use_structured_ranked_path, 'r'))\n",
    "            sr_names = [x[0] for x in sr]\n",
    "            candidate_layers = [n for n in sr_names if n in conv_names]\n",
    "            print(f\"Loaded structured ranking, candidate conv layers (top {ranked_k}): {candidate_layers[:ranked_k]}\")\n",
    "        except Exception as e:\n",
    "            print(\"Failed to load structured ranked file:\", e)\n",
    "\n",
    "    if candidate_layers is None and use_unstructured_ranked_path and os.path.exists(use_unstructured_ranked_path):\n",
    "        try:\n",
    "            ur = json.load(open(use_unstructured_ranked_path, 'r'))\n",
    "            ur_names = [x[0] for x in ur]\n",
    "            candidate_layers = [n for n in ur_names if n in conv_names]\n",
    "            print(f\"Loaded unstructured ranking, candidate conv layers (top {ranked_k}): {candidate_layers[:ranked_k]}\")\n",
    "        except Exception as e:\n",
    "            print(\"Failed to load unstructured ranked file:\", e)\n",
    "\n",
    "    if candidate_layers is None:\n",
    "        candidate_layers = conv_names.copy()\n",
    "        print(\"No ranking files found; defaulting to all conv layers as candidates.\")\n",
    "\n",
    "    prune_candidates = candidate_layers[:ranked_k] if ranked_k is not None else candidate_layers\n",
    "    print(\"Prune candidate convs used:\", prune_candidates)\n",
    "\n",
    "    kept_map = {}\n",
    "    for name, conv_mod in convs:\n",
    "        requested_frac = float(per_layer_sparsities.get(name, 0.0))\n",
    "        if name in prune_candidates:\n",
    "            spars = requested_frac\n",
    "        else:\n",
    "            spars = 0.0\n",
    "        keep_frac = max(0.0, 1.0 - spars)\n",
    "        kept_map[name] = choose_kept_channels(conv_mod, keep_frac)\n",
    "\n",
    "    print(\"Per-layer kept channels (orig -> kept):\")\n",
    "    for name, conv_mod in convs:\n",
    "        orig_ch = conv_mod.out_channels\n",
    "        kept_ch = len(kept_map[name])\n",
    "        print(f\"  {name}: {orig_ch} -> {kept_ch}  (prune frac requested {per_layer_sparsities.get(name,0.0):.3f})\")\n",
    "\n",
    "    pruned_model, kept_used = build_pruned_vgg(kept_map)\n",
    "    pruned_model.to(device)\n",
    "    pruned_model.eval()\n",
    "\n",
    "    acc_before = evaluate(pruned_model, CIFAR10_testloader)\n",
    "    print(f\"[structured_final] val acc immediately after pruning: {acc_before:.2f}\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(pruned_model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "    best_val = acc_before\n",
    "    no_imp = 0\n",
    "    for ep in range(finetune_epochs):\n",
    "        train_one_epoch(pruned_model, CIFAR10_trainloader, criterion, optimizer)\n",
    "        val_acc = evaluate(pruned_model, CIFAR10_testloader)\n",
    "        print(f\"[structured_final] finetune epoch {ep+1}/{finetune_epochs} val_acc={val_acc:.2f}\")\n",
    "        if val_acc > best_val + 1e-6:\n",
    "            best_val = val_acc\n",
    "            no_imp = 0\n",
    "        else:\n",
    "            no_imp += 1\n",
    "        if no_imp >= 2:\n",
    "            print(\"Early stopping: no improvement in 2 epochs.\")\n",
    "            break\n",
    "\n",
    "    save_path = os.path.join(out_dir, 'structured_pruned_final.pth')\n",
    "\n",
    "    def param_count(m): return sum(p.numel() for p in m.parameters())\n",
    "    orig_params = param_count(vgg_model)\n",
    "    new_params = param_count(pruned_model)\n",
    "    global_sparsity = 1.0 - (new_params / orig_params) if orig_params>0 else 0.0\n",
    "    summary = {'orig_params': orig_params, 'pruned_params': new_params, 'global_sparsity': global_sparsity}\n",
    "    with open(os.path.join(out_dir, 'structured_prune_summary.json'), 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "    pruned_sd_cpu = {k: v.cpu() for k,v in pruned_model.state_dict().items()}\n",
    "    torch.save({\n",
    "        'model_state_dict': pruned_sd_cpu,\n",
    "        'num_classes': num_classes,\n",
    "        'arch': 'vgg11_bn',\n",
    "        'kept_indices': kept_used,\n",
    "        'structured_per_layer_sparsities': per_layer_sparsities,  \n",
    "        'global_sparsity': summary['global_sparsity']\n",
    "    }, save_path) \n",
    "    print(\"Saved final structured state_dict:\", save_path)\n",
    "    print(f\"Saved final structured pruned model to {save_path}. Global sparsity (by params): {global_sparsity:.4f}\")\n",
    "    return pruned_model, kept_used, summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vt_hy9-D8Cvx",
    "outputId": "c1cc0e4f-c0c4-4d13-9579-113e2f729428"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded structured ranking, candidate conv layers (top 8): ['features.25', 'features.22', 'features.18', 'features.0', 'features.15', 'features.8', 'features.11', 'features.4']\n",
      "Prune candidate convs used: ['features.25', 'features.22', 'features.18', 'features.0', 'features.15', 'features.8', 'features.11', 'features.4']\n",
      "Per-layer kept channels (orig -> kept):\n",
      "  features.0: 64 -> 7  (prune frac requested 0.891)\n",
      "  features.4: 128 -> 13  (prune frac requested 0.898)\n",
      "  features.8: 256 -> 26  (prune frac requested 0.898)\n",
      "  features.11: 256 -> 26  (prune frac requested 0.898)\n",
      "  features.15: 512 -> 52  (prune frac requested 0.898)\n",
      "  features.18: 512 -> 52  (prune frac requested 0.898)\n",
      "  features.22: 512 -> 52  (prune frac requested 0.898)\n",
      "  features.25: 512 -> 52  (prune frac requested 0.898)\n",
      "[structured_final] val acc immediately after pruning: 10.00\n",
      "[structured_final] finetune epoch 1/10 val_acc=44.71\n",
      "[structured_final] finetune epoch 2/10 val_acc=57.72\n",
      "[structured_final] finetune epoch 3/10 val_acc=55.70\n",
      "[structured_final] finetune epoch 4/10 val_acc=62.65\n",
      "[structured_final] finetune epoch 5/10 val_acc=61.21\n",
      "[structured_final] finetune epoch 6/10 val_acc=66.60\n",
      "[structured_final] finetune epoch 7/10 val_acc=68.21\n",
      "[structured_final] finetune epoch 8/10 val_acc=68.89\n",
      "[structured_final] finetune epoch 9/10 val_acc=71.02\n",
      "[structured_final] finetune epoch 10/10 val_acc=70.02\n",
      "Saved final structured state_dict: structured_final/structured_pruned_final.pth\n",
      "Saved final structured pruned model to structured_final/structured_pruned_final.pth. Global sparsity (by params): 0.7876\n",
      "Structured summary: {'orig_params': 128812810, 'pruned_params': 27359136, 'global_sparsity': 0.7876054718470935}\n"
     ]
    }
   ],
   "source": [
    "pruned_model, kept_used, summary = apply_structured_and_finetune(per_layer_sparsities, finetune_epochs=10, out_dir='structured_final')\n",
    "print(\"Structured summary:\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T19:50:13.858543Z",
     "iopub.status.busy": "2025-11-01T19:50:13.858207Z",
     "iopub.status.idle": "2025-11-01T19:50:22.132260Z",
     "shell.execute_reply": "2025-11-01T19:50:22.131438Z",
     "shell.execute_reply.started": "2025-11-01T19:50:13.858515Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: total weight elements = 128,799,104 > 5,000,000; sampling for plotting.\n",
      "Note: total weight elements = 27,350,374 > 5,000,000; sampling for plotting.\n",
      "Saved weight distribution comparison to weight_dists_struct.png\n"
     ]
    }
   ],
   "source": [
    "pruned_model_s, kept_map_s, ck_s = load_structured_checkpoint('structured-f/structured_pruned_final.pth', device=device)\n",
    "plot_weight_distributions(\"baseline_vgg11.pth\", \"/kaggle/input/structured-f/structured_pruned_final.pth\", \n",
    "                          out_path='weight_dists_struct.png', bins=200, log_scale=True, per_layer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T21:33:12.213246Z",
     "iopub.status.busy": "2025-11-01T21:33:12.212468Z",
     "iopub.status.idle": "2025-11-01T21:33:12.356113Z",
     "shell.execute_reply": "2025-11-01T21:33:12.355378Z",
     "shell.execute_reply.started": "2025-11-01T21:33:12.213220Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ranked plot to sensitivity_vis_struct/sensitivity_ranked_struct.png\n",
      "Top 5 most sensitive layers:\n",
      "  features.25                     drop=74.16\n",
      "  features.22                     drop=70.53\n",
      "  features.8                      drop=50.41\n",
      "  features.15                     drop=47.43\n",
      "  features.0                      drop=40.74\n",
      "Top 5 least sensitive layers:\n",
      "  features.15                     drop=47.43\n",
      "  features.0                      drop=40.74\n",
      "  features.11                     drop=33.57\n",
      "  features.4                      drop=32.95\n",
      "  features.18                     drop=32.12\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ranked_path = \"/kaggle/working/structured_quick/ranked_layers_by_noft.json\" \n",
    "out_dir = \"sensitivity_vis_struct\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "with open(ranked_path, \"r\") as f:\n",
    "    ranked_layers = json.load(f)\n",
    "\n",
    "if isinstance(ranked_layers, dict):\n",
    "    items = list(ranked_layers.items())\n",
    "elif isinstance(ranked_layers, list):\n",
    "    items = [(k, float(v)) for k, v in ranked_layers]\n",
    "else:\n",
    "    raise RuntimeError(f\"Unexpected type: {type(ranked_layers)}\")\n",
    "\n",
    "items = sorted(items, key=lambda x: x[1], reverse=True)  \n",
    "\n",
    "layers = [k for k, _ in items]\n",
    "drops  = [v for _, v in items]\n",
    "\n",
    "topN = min(40, len(layers))\n",
    "plt.figure(figsize=(8, max(3, 0.25 * topN)))\n",
    "plt.barh(np.arange(topN)[::-1], drops[:topN])\n",
    "plt.yticks(np.arange(topN), layers[:topN][::-1])\n",
    "plt.xlabel(\"Average accuracy drop (%)\")\n",
    "plt.title(\"Layer sensitivity ranking (higher = more sensitive)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(out_dir, \"sensitivity_ranked_from_json.png\"))\n",
    "plt.close()\n",
    "print(f\"Saved ranked plot to {os.path.join(out_dir, 'sensitivity_ranked_struct.png')}\")\n",
    "\n",
    "print(\"Top 5 most sensitive layers:\")\n",
    "for k, v in items[:5]:\n",
    "    print(f\"  {k:30s}  drop={v:.2f}\")\n",
    "print(\"Top 5 least sensitive layers:\")\n",
    "for k, v in items[-5:]:\n",
    "    print(f\"  {k:30s}  drop={v:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRU6AgZE5Oop"
   },
   "source": [
    "*Part 3*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T19:55:56.360658Z",
     "iopub.status.busy": "2025-11-01T19:55:56.360040Z",
     "iopub.status.idle": "2025-11-01T19:55:56.376763Z",
     "shell.execute_reply": "2025-11-01T19:55:56.376040Z",
     "shell.execute_reply.started": "2025-11-01T19:55:56.360635Z"
    },
    "id": "a3Pa3-Ek5P5m",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_vgg_from_checkpoint(ckpt_path, device=None, prefer_state_key='model_state_dict'):\n",
    "\n",
    "    if device is None:\n",
    "        try:\n",
    "            device = globals().get('device', 'cpu')\n",
    "        except Exception:\n",
    "            device = 'cpu'\n",
    "    device = torch.device(device)\n",
    "\n",
    "    if not os.path.exists(ckpt_path):\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {ckpt_path}\")\n",
    "\n",
    "    ck = torch.load(ckpt_path, map_location='cpu')\n",
    "\n",
    "    if isinstance(ck, dict) and prefer_state_key in ck:\n",
    "        state = ck[prefer_state_key]\n",
    "    elif isinstance(ck, dict) and 'state_dict' in ck:\n",
    "        state = ck['state_dict']\n",
    "    else:\n",
    "        state = ck\n",
    "\n",
    "    if not isinstance(state, dict):\n",
    "        raise RuntimeError(\"Loaded checkpoint does not contain a state-dict mapping.\")\n",
    "\n",
    "    num_classes = None\n",
    "    if 'classifier.6.weight' in state:\n",
    "        w = state['classifier.6.weight']\n",
    "        try:\n",
    "            num_classes = int(w.shape[0])\n",
    "        except Exception:\n",
    "            pass\n",
    "    if num_classes is None and 'classifier.6.bias' in state:\n",
    "        b = state['classifier.6.bias']\n",
    "        try:\n",
    "            num_classes = int(b.shape[0])\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if num_classes is None:\n",
    "        for k, v in state.items():\n",
    "            if k.endswith('classifier.6.weight') or k.endswith('classifier.6.bias'):\n",
    "                try:\n",
    "                    num_classes = int(state[k].shape[0])\n",
    "                    break\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "    if num_classes is None:\n",
    "        raise RuntimeError(\n",
    "            \"Could not infer classifier output dimension from checkpoint. \"\n",
    "            \"Please pass a checkpoint that contains 'classifier.6.weight' or provide a cleaned state_dict.\"\n",
    "        )\n",
    "\n",
    "    model = torchvision.models.vgg11_bn(pretrained=False)\n",
    "    in_features = model.classifier[6].in_features  \n",
    "    if model.classifier[6].out_features != num_classes:\n",
    "        model.classifier[6] = nn.Linear(in_features=in_features, out_features=num_classes, bias=True)\n",
    "    new_state = {}\n",
    "    for k, v in state.items():\n",
    "        new_k = k\n",
    "        if k.startswith('module.'):\n",
    "            new_k = k[len('module.'):]\n",
    "        new_state[new_k] = v\n",
    "\n",
    "    try:\n",
    "        model.load_state_dict(new_state, strict=True)\n",
    "    except RuntimeError as e:\n",
    "        err_msg = str(e)\n",
    "        print(\"Strict load failed:\", err_msg)\n",
    "        load_res = model.load_state_dict(new_state, strict=False)  \n",
    "        print(\"Loaded state_dict with strict=False. Missing keys:\", load_res.missing_keys)\n",
    "        print(\"Unexpected keys:\", load_res.unexpected_keys)\n",
    "        mismatches = []\n",
    "        for k, v in new_state.items():\n",
    "            if k in dict(model.named_parameters()) or k in dict(model.named_buffers()):\n",
    "                try:\n",
    "                    model_tensor = dict(model.named_parameters()).get(k, None)\n",
    "                    if model_tensor is None:\n",
    "                        model_tensor = dict(model.named_buffers()).get(k, None)\n",
    "                    if model_tensor is not None:\n",
    "                        if tuple(model_tensor.shape) != tuple(v.shape):\n",
    "                            mismatches.append((k, tuple(v.shape), tuple(model_tensor.shape)))\n",
    "                except Exception:\n",
    "                    pass\n",
    "        if mismatches:\n",
    "            print(\"Parameter shape mismatches (loaded_shape -> model_shape):\")\n",
    "            for k, ls, ms in mismatches:\n",
    "                print(f\"  {k}: {ls} -> {ms}\")\n",
    "        else:\n",
    "            print(\"No parameter shape mismatches detected beyond missing/unexpected keys.\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(f\"Loaded VGG from checkpoint '{ckpt_path}' with classifier out_features = {num_classes}\")\n",
    "    return model\n",
    "\n",
    "def get_module_by_qname(model, qname):\n",
    "    parts = qname.split('.')\n",
    "    mod = model\n",
    "    for p in parts:\n",
    "        if p.isdigit():\n",
    "            mod = mod[int(p)]\n",
    "        else:\n",
    "            if not hasattr(mod, p):\n",
    "                raise AttributeError(f\"Module {mod} has no attribute {p} (qname={qname})\")\n",
    "            mod = getattr(mod, p)\n",
    "    return mod\n",
    "\n",
    "def get_default_target_layer_name(model):\n",
    "    last_idx = None\n",
    "    for idx, module in enumerate(model.features):\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            last_idx = idx\n",
    "    if last_idx is None:\n",
    "        raise RuntimeError(\"No Conv2d found in model.features\")\n",
    "    return f\"features.{last_idx}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T19:55:59.638210Z",
     "iopub.status.busy": "2025-11-01T19:55:59.637951Z",
     "iopub.status.idle": "2025-11-01T19:55:59.650584Z",
     "shell.execute_reply": "2025-11-01T19:55:59.649734Z",
     "shell.execute_reply.started": "2025-11-01T19:55:59.638191Z"
    },
    "id": "NzpSSiEO5T7A",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_gradcam(model, input_tensor, target_layer_name=None, class_idx=None, device=device):\n",
    "    model.eval()\n",
    "    if target_layer_name is None:\n",
    "        target_layer_name = get_default_target_layer_name(model)\n",
    "\n",
    "    activations = None\n",
    "    gradients = None\n",
    "\n",
    "    def forward_hook(module, inp, out):\n",
    "        nonlocal activations\n",
    "        activations = out.detach()\n",
    "\n",
    "    def backward_hook(module, grad_input, grad_output):\n",
    "        nonlocal gradients\n",
    "        gradients = grad_output[0].detach()\n",
    "\n",
    "    target_module = get_module_by_qname(model, target_layer_name)\n",
    "    fh = target_module.register_forward_hook(forward_hook)\n",
    "    try:\n",
    "        bh = target_module.register_full_backward_hook(backward_hook)\n",
    "    except AttributeError:\n",
    "        bh = target_module.register_backward_hook(backward_hook)\n",
    "\n",
    "    input_tensor = input_tensor.to(device)\n",
    "    model.to(device)\n",
    "    input_tensor.requires_grad_(True)\n",
    "\n",
    "    out = model(input_tensor)             \n",
    "    probs = F.softmax(out, dim=1)\n",
    "    pred_class = int(probs.argmax(dim=1).item())\n",
    "    if class_idx is None:\n",
    "        class_idx = pred_class\n",
    "\n",
    "    model.zero_grad()\n",
    "    score = out[0, class_idx]\n",
    "    score.backward(retain_graph=False)\n",
    "\n",
    "    if activations is None or gradients is None:\n",
    "        fh.remove(); bh.remove()\n",
    "        raise RuntimeError(\"Hooks did not capture activations/gradients. Check target_layer_name.\")\n",
    "\n",
    "    weights = gradients.mean(dim=(2,3), keepdim=False).squeeze(0)  \n",
    "    cam = (weights.view(-1,1,1) * activations[0]).sum(dim=0)    \n",
    "    cam = F.relu(cam)\n",
    "    cam_np = cam.cpu().numpy()\n",
    "    cam_np -= cam_np.min()\n",
    "    if cam_np.max() > 0:\n",
    "        cam_np = cam_np / cam_np.max()\n",
    "\n",
    "    _, _, H_in, W_in = input_tensor.shape\n",
    "    cam_tensor = torch.from_numpy(cam_np).unsqueeze(0).unsqueeze(0).float()  # (1,1,Hf,Wf)\n",
    "    cam_resized = F.interpolate(cam_tensor, size=(H_in, W_in), mode='bilinear', align_corners=False)\n",
    "    cam_resized = cam_resized.squeeze().cpu().numpy()\n",
    "    fh.remove(); bh.remove()\n",
    "    return cam_resized, pred_class\n",
    "\n",
    "def overlay_and_save(original_img_tensor, heatmap, out_path, alpha=0.5):\n",
    "    mean = np.array([0.4914, 0.4822, 0.4465])\n",
    "    std  = np.array([0.2470, 0.2435, 0.2616])\n",
    "    img = original_img_tensor.detach().cpu().numpy().transpose(1,2,0)  # H,W,C normalized\n",
    "    img = (img * std + mean)\n",
    "    img = np.clip(img, 0, 1)\n",
    "\n",
    "    H, W = img.shape[0], img.shape[1]\n",
    "    if heatmap.shape != (H, W):\n",
    "        heatmap_img = Image.fromarray(np.uint8(heatmap * 255))\n",
    "        heatmap_img = heatmap_img.resize((W, H), resample=Image.BILINEAR)\n",
    "        heatmap_resized = np.array(heatmap_img) / 255.0\n",
    "    else:\n",
    "        heatmap_resized = heatmap\n",
    "\n",
    "    cmap = plt.get_cmap('jet')\n",
    "    heat_col = cmap(heatmap_resized)[:, :, :3]  \n",
    "    overlay = (1-alpha) * img + alpha * heat_col\n",
    "    overlay = np.clip(overlay, 0, 1)\n",
    "\n",
    "    fig, axs = plt.subplots(1,3, figsize=(9,3))\n",
    "    axs[0].imshow(img); axs[0].set_title(\"Original\"); axs[0].axis('off')\n",
    "    axs[1].imshow(heatmap_resized, cmap='jet'); axs[1].set_title(\"Grad-CAM\"); axs[1].axis('off')\n",
    "    axs[2].imshow(overlay); axs[2].set_title(\"Overlay\"); axs[2].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T20:44:41.471223Z",
     "iopub.status.busy": "2025-11-01T20:44:41.470977Z",
     "iopub.status.idle": "2025-11-01T20:44:41.483565Z",
     "shell.execute_reply": "2025-11-01T20:44:41.482654Z",
     "shell.execute_reply.started": "2025-11-01T20:44:41.471207Z"
    },
    "id": "Dzyb-e5x5ZMV",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compare_gradcam_models(models_info, image_indices=[0], target_layer_name=None, out_dir='gradcam_compare', device=None):\n",
    " \n",
    "    if device is None:\n",
    "        device = globals().get('device', torch.device('cpu'))\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    label_model_map = {}\n",
    "    for label, info in models_info.items():\n",
    "        if isinstance(info, tuple) and info[0] in ('original','unstructured','structured'):\n",
    "            typ, path = info\n",
    "            if typ == 'original':\n",
    "                m = vgg_model\n",
    "                m.to(device)\n",
    "                m.eval()\n",
    "                label_model_map[label] = m\n",
    "            else:\n",
    "                if path is None or (isinstance(path, str) and not os.path.exists(path)):\n",
    "                    raise ValueError(f\"Checkpoint path required & must exist for model label {label}: got {path}\")\n",
    "                ck = torch.load(path, map_location='cpu')\n",
    "                if isinstance(ck, dict) and 'kept_indices' in ck:\n",
    "                    m, kept, ckmeta = load_structured_checkpoint(path, device=device)   \n",
    "                    label_model_map[label] = m\n",
    "                else:\n",
    "                    m, ckmeta = load_unstructured_checkpoint(path, device=device)     \n",
    "                    label_model_map[label] = m\n",
    "        elif isinstance(info, nn.Module):\n",
    "            label_model_map[label] = info.to(device)\n",
    "        elif isinstance(info, str) and os.path.exists(info):\n",
    "            ck = torch.load(info, map_location='cpu')\n",
    "            if isinstance(ck, dict) and 'kept_indices' in ck:\n",
    "                m, kept, ckmeta = load_structured_checkpoint(info, device=device)\n",
    "                label_model_map[label] = m\n",
    "            else:\n",
    "                m, ckmeta = load_unstructured_checkpoint(info, device=device)\n",
    "                label_model_map[label] = m\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported models_info entry for '{label}': {info}\")\n",
    "\n",
    "    dataset = CIFAR10_testset\n",
    "    for idx in image_indices:\n",
    "        img, gt = dataset[idx]\n",
    "        inp = img.unsqueeze(0).to(device)   \n",
    "        for label, model in label_model_map.items():\n",
    "            try:\n",
    "                cam, pred = compute_gradcam(model, inp.clone(), target_layer_name=target_layer_name, class_idx=None, device=device)\n",
    "            except Exception as e:\n",
    "                print(f\"[GradCAM] failed for {label} idx {idx}: {e}\")\n",
    "                continue\n",
    "            out_path = os.path.join(out_dir, f\"{label}_img{idx}.png\")\n",
    "            overlay_and_save(img, cam, out_path)  \n",
    "            print(f\"[GradCAM] saved {out_path} (pred {pred})\")\n",
    "    print(\"Saved Grad-CAM comparisons to\", out_dir)\n",
    "\n",
    "def measure_inference_time(model, input_tensor, runs=200, warmup=20, device=None):\n",
    "    if device is None:\n",
    "        device = globals().get('device', torch.device('cpu'))\n",
    "    device = torch.device(device)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    inp = input_tensor.to(device)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(warmup):\n",
    "            _ = model(inp)\n",
    "    times = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(runs):\n",
    "            t0 = time.time()\n",
    "            _ = model(inp)\n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "            t1 = time.time()\n",
    "            times.append((t1 - t0) * 1000.0)\n",
    "    times = np.array(times)\n",
    "    return float(times.mean()), float(times.std()), times.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T19:56:04.237642Z",
     "iopub.status.busy": "2025-11-01T19:56:04.236954Z",
     "iopub.status.idle": "2025-11-01T19:56:04.246299Z",
     "shell.execute_reply": "2025-11-01T19:56:04.245631Z",
     "shell.execute_reply.started": "2025-11-01T19:56:04.237615Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_checkpoint_auto(ckpt_path, device=None):\n",
    "    if device is None:\n",
    "        device = globals().get('device', torch.device('cpu'))\n",
    "    device = torch.device(device)\n",
    "\n",
    "    if not os.path.exists(ckpt_path):\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {ckpt_path}\")\n",
    "\n",
    "    ck = torch.load(ckpt_path, map_location='cpu')\n",
    "\n",
    "    if not isinstance(ck, dict):\n",
    "        try:\n",
    "            m = torchvision.models.vgg11_bn(pretrained=False)\n",
    "            return load_vgg_from_checkpoint(ckpt_path, device=device), {}\n",
    "        except Exception:\n",
    "            raise RuntimeError(\"Unable to load checkpoint (not dict and not a simple state_dict).\")\n",
    "\n",
    "    if 'kept_indices' in ck or 'kept_indices' in ck.get('model_state_dict', {}):\n",
    "        try:\n",
    "            model, kept, meta = load_structured_checkpoint(ckpt_path, device=device)\n",
    "            return model, meta\n",
    "        except NameError:\n",
    "            kept_indices = ck.get('kept_indices') or ck.get('model_state_dict', {}).get('kept_indices')\n",
    "            if kept_indices is None:\n",
    "                raise RuntimeError(\"kept_indices present but loader function missing.\")\n",
    "            pruned_model, _ = build_pruned_vgg(kept_indices)\n",
    "            pruned_model.to(device)\n",
    "            sd = ck.get('model_state_dict', None) or ck\n",
    "            try:\n",
    "                pruned_model.load_state_dict(sd, strict=False)\n",
    "            except Exception as e:\n",
    "                print(\"Warning: loading structured checkpoint with strict=False:\", e)\n",
    "            pruned_model.eval()\n",
    "            return pruned_model, ck\n",
    "    try:\n",
    "        model = load_vgg_from_checkpoint(ckpt_path, device=device)\n",
    "        return model, ck\n",
    "    except Exception as e:\n",
    "        m = torchvision.models.vgg11_bn(pretrained=False)\n",
    "        if isinstance(ck, dict):\n",
    "            state = ck.get('model_state_dict', ck)\n",
    "            if 'classifier.6.weight' in state:\n",
    "                try:\n",
    "                    outdim = int(state['classifier.6.weight'].shape[0])\n",
    "                    m.classifier[6] = nn.Linear(m.classifier[6].in_features, outdim)\n",
    "                except Exception:\n",
    "                    pass\n",
    "        try:\n",
    "            m.load_state_dict(state, strict=False)\n",
    "            m.to(device); m.eval()\n",
    "            return m, ck\n",
    "        except Exception as e2:\n",
    "            raise RuntimeError(f\"Failed to auto-load checkpoint {ckpt_path}: {e2}\") from e2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T19:56:06.469153Z",
     "iopub.status.busy": "2025-11-01T19:56:06.468895Z",
     "iopub.status.idle": "2025-11-01T19:56:06.476210Z",
     "shell.execute_reply": "2025-11-01T19:56:06.475441Z",
     "shell.execute_reply.started": "2025-11-01T19:56:06.469133Z"
    },
    "id": "ifXvs8_z5c3M",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def model_checkpoint_size_bytes(ckpt_path):\n",
    "    if not os.path.exists(ckpt_path):\n",
    "        return None\n",
    "    return os.path.getsize(ckpt_path)\n",
    "\n",
    "def compute_sparsity_from_state_dict(state_dict):\n",
    "    total = 0\n",
    "    nz = 0\n",
    "    for k, v in state_dict.items():\n",
    "        if 'weight' not in k:\n",
    "            continue\n",
    "        try:\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                arr = v.detach().cpu().numpy()\n",
    "            else:\n",
    "                arr = np.asarray(v)\n",
    "        except Exception:\n",
    "            continue\n",
    "        total += arr.size\n",
    "        nz += np.count_nonzero(arr)\n",
    "    if total == 0:\n",
    "        return None\n",
    "    return 1.0 - (nz / total)\n",
    "\n",
    "def evaluate_checkpoint_accuracy(ckpt_path, batch_size=128):\n",
    "    if not os.path.exists(ckpt_path):\n",
    "        raise FileNotFoundError(ckpt_path)\n",
    "    model, ckmeta = load_checkpoint_auto(ckpt_path, device=device)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x,y in CIFAR10_testloader:\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            out = model(x)\n",
    "            _, pred = out.max(1)\n",
    "            correct += pred.eq(y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return 100.0 * correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T19:56:09.988874Z",
     "iopub.status.busy": "2025-11-01T19:56:09.988344Z",
     "iopub.status.idle": "2025-11-01T19:56:10.002731Z",
     "shell.execute_reply": "2025-11-01T19:56:10.001990Z",
     "shell.execute_reply.started": "2025-11-01T19:56:09.988847Z"
    },
    "id": "TBXks5pG5iQ0",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def accuracy_vs_sparsity_plot(readings, out_path='acc_vs_spars.png'):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    notes = []\n",
    "    for s, ck in readings:\n",
    "      s_pct = float(s)*100.0 if (0.0 <= float(s) <= 1.0) else float(s)\n",
    "      if not os.path.exists(ck):\n",
    "          print(f\"Skipping missing checkpoint {ck}\")\n",
    "          continue\n",
    "      try:\n",
    "          acc = evaluate_checkpoint_accuracy(ck)\n",
    "      except Exception as e:\n",
    "          print(f\"Failed to eval {ck}: {e}\")\n",
    "          continue\n",
    "      xs.append(s_pct); ys.append(acc); notes.append((s_pct, acc))\n",
    "      print(f\"  -> accuracy: {acc:.2f}%\")\n",
    "    order = np.argsort(xs)\n",
    "    xs_sorted = np.array(xs)[order]\n",
    "    ys_sorted = np.array(ys)[order]\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(xs_sorted, ys_sorted, marker='o')\n",
    "    plt.xlabel('Target sparsity (%)')\n",
    "    plt.ylabel('Validation accuracy (%)')\n",
    "    plt.title('Accuracy vs target sparsity')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path)\n",
    "    plt.close()\n",
    "    print(\"Saved accuracy vs sparsity plot to\", out_path)\n",
    "    return notes\n",
    "\n",
    "def summarize_models(original_label, original_model_or_ckpt, unstructured_label, unstructured_ckpt, structured_label, \n",
    "                     structured_ckpt, sample_image_index=0, runs=200, out_dir='model_summary'):\n",
    "  \n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    def load_entry(entry):\n",
    "      if isinstance(entry, nn.Module):\n",
    "          return entry\n",
    "      if entry == 'original':\n",
    "          return vgg_model\n",
    "      if isinstance(entry, str) and os.path.exists(entry):\n",
    "          m, ckmeta = load_checkpoint_auto(entry, device=device)\n",
    "          return m\n",
    "      raise ValueError(\"Entry must be 'original' or path or nn.Module\")\n",
    "\n",
    "\n",
    "    models = {}\n",
    "    ckpt_paths = {'original': None, 'unstructured': None, 'structured': None}\n",
    "    models[original_label] = load_entry(original_model_or_ckpt)\n",
    "    models[unstructured_label] = load_entry(unstructured_ckpt)\n",
    "    models[structured_label] = load_entry(structured_ckpt)\n",
    "    if isinstance(original_model_or_ckpt, str) and os.path.exists(original_model_or_ckpt):\n",
    "        ckpt_paths['original'] = original_model_or_ckpt\n",
    "    if os.path.exists(unstructured_ckpt):\n",
    "        ckpt_paths['unstructured'] = unstructured_ckpt\n",
    "    if os.path.exists(structured_ckpt):\n",
    "        ckpt_paths['structured'] = structured_ckpt\n",
    "\n",
    "    img, _ = CIFAR10_testset[sample_image_index]\n",
    "    input_tensor = img.unsqueeze(0)\n",
    "\n",
    "    summary = {}\n",
    "    for label, model in models.items():\n",
    "        mean_ms, std_ms, _ = measure_inference_time(model, input_tensor, runs=runs)\n",
    "        param_cnt = sum(p.numel() for p in model.parameters())\n",
    "        ckpt_size = None\n",
    "        ckpt_path = None\n",
    "        if label == original_label:\n",
    "            ckpt_path = original_model_or_ckpt if isinstance(original_model_or_ckpt, str) and os.path.exists(original_model_or_ckpt) else None\n",
    "        elif label == unstructured_label:\n",
    "            ckpt_path = unstructured_ckpt\n",
    "        elif label == structured_label:\n",
    "            ckpt_path = structured_ckpt\n",
    "        if ckpt_path and os.path.exists(ckpt_path):\n",
    "            ckpt_size = os.path.getsize(ckpt_path)\n",
    "            ck = torch.load(ckpt_path, map_location='cpu')\n",
    "            if 'model_state_dict' in ck:\n",
    "                sd = ck['model_state_dict']\n",
    "            elif 'state_dict' in ck:\n",
    "                sd = ck['state_dict']\n",
    "            else:\n",
    "                sd = ck\n",
    "            spars = compute_sparsity_from_state_dict(sd)\n",
    "        else:\n",
    "            spars = None\n",
    "        summary[label] = {\n",
    "            'mean_inference_ms': mean_ms,\n",
    "            'std_inference_ms': std_ms,\n",
    "            'param_count': param_cnt,\n",
    "            'ckpt_path': ckpt_path,\n",
    "            'ckpt_size_bytes': ckpt_size,\n",
    "            'computed_sparsity': spars\n",
    "        }\n",
    "\n",
    "    summary_serializable = {}\n",
    "    for label, info in summary.items():\n",
    "        summary_serializable[label] = {\n",
    "            'mean_inference_ms': float(info['mean_inference_ms']),\n",
    "            'std_inference_ms': float(info['std_inference_ms']),\n",
    "            'param_count': int(info['param_count']),\n",
    "            'ckpt_path': info['ckpt_path'],\n",
    "            'ckpt_size_bytes': int(info['ckpt_size_bytes']) if info['ckpt_size_bytes'] is not None else None,\n",
    "            'computed_sparsity': float(info['computed_sparsity']) if info['computed_sparsity'] is not None else None\n",
    "        }\n",
    "    with open(os.path.join(out_dir, 'models_summary.json'), 'w') as f:\n",
    "        json.dump(summary_serializable, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 616
    },
    "execution": {
     "iopub.execute_input": "2025-11-01T20:00:40.202270Z",
     "iopub.status.busy": "2025-11-01T20:00:40.201687Z",
     "iopub.status.idle": "2025-11-01T20:00:40.205732Z",
     "shell.execute_reply": "2025-11-01T20:00:40.204903Z",
     "shell.execute_reply.started": "2025-11-01T20:00:40.202248Z"
    },
    "id": "UC3kqOLlEbpw",
    "outputId": "47f21c9e-bad6-4ee4-f785-1d7107682e2a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "baseline_ckpt = \"baseline_vgg11.pth\"\n",
    "unstructured_ckpt = \"unstructured_outputs/final/final_pruned_state_dict.pth\"   \n",
    "structured_ckpt   = \"structured_final/structured_pruned_final_state_dict.pth\"  \n",
    "models_info = {\n",
    "    \"baseline\": (\"original\", None),\n",
    "    \"unstructured\": (\"unstructured\", unstructured_ckpt),\n",
    "    \"structured\": (\"structured\", structured_ckpt)\n",
    "}\n",
    "image_indices = [0, 7, 42]\n",
    "compare_gradcam_models(models_info, image_indices=image_indices, target_layer_name=None, out_dir='gradcam_compare', device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T20:03:09.926354Z",
     "iopub.status.busy": "2025-11-01T20:03:09.925663Z",
     "iopub.status.idle": "2025-11-01T20:03:09.929845Z",
     "shell.execute_reply": "2025-11-01T20:03:09.929026Z",
     "shell.execute_reply.started": "2025-11-01T20:03:09.926328Z"
    },
    "id": "VnyPERxTEj_q",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "summary = summarize_models(\n",
    "    original_label=\"baseline\", original_model_or_ckpt='original',\n",
    "    unstructured_label=\"unstructured\", unstructured_ckpt=unstructured_ckpt,\n",
    "    structured_label=\"structured\",   structured_ckpt=structured_ckpt,\n",
    "    sample_image_index=0,   \n",
    "    runs=200,               \n",
    "    out_dir='model_summary'\n",
    ")\n",
    "print(json.dumps(summary, indent=2))\n",
    "\n",
    "print(\"Baseline ckpt size:\", model_checkpoint_size_bytes(baseline_ckpt))\n",
    "print(\"Unstructured ckpt size:\", model_checkpoint_size_bytes(unstructured_ckpt))\n",
    "print(\"Structured ckpt size:\", model_checkpoint_size_bytes(structured_ckpt))\n",
    "\n",
    "ck = torch.load(unstructured_ckpt, map_location='cpu')\n",
    "sd = ck.get('model_state_dict', ck)\n",
    "print(\"Unstructured computed sparsity:\", compute_sparsity_from_state_dict(sd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GJ8BsEzsEsBx"
   },
   "outputs": [],
   "source": [
    "readings = [\n",
    "    (0.0, \"baseline_vgg11.pth\"),\n",
    "    (0.3, \"unstructured_outputs/ckpt_s30.pth\"),\n",
    "    (0.5, \"unstructured_outputs/ckpt_s50.pth\"),\n",
    "    (0.7, \"unstructured_outputs/ckpt_s70.pth\"),\n",
    "    (0.9, \"unstructured_outputs/ckpt_s90.pth\"),\n",
    "]\n",
    "\n",
    "notes = accuracy_vs_sparsity_plot(readings, out_path='acc_vs_spars.png')\n",
    "print(\"Readings (sparsity_pct, acc):\", notes)\n",
    "\n",
    "\n",
    "img, _ = CIFAR10_testset[0]\n",
    "inp = img.unsqueeze(0)\n",
    "m = load_vgg_from_checkpoint(unstructured_ckpt)  # or pruned model object\n",
    "mean_ms, std_ms, all_times = measure_inference_time(m, inp, runs=200, warmup=20)\n",
    "print(\"Mean ms:\", mean_ms, \"std ms:\", std_ms)\n",
    "\n",
    "cam, pred = compute_gradcam(m, inp, target_layer_name=None, class_idx=None, device=device)\n",
    "overlay_and_save(img, cam, out_path='single_gradcam_unstructured.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T20:53:35.867201Z",
     "iopub.status.busy": "2025-11-01T20:53:35.866903Z",
     "iopub.status.idle": "2025-11-01T20:53:49.917742Z",
     "shell.execute_reply": "2025-11-01T20:53:49.916919Z",
     "shell.execute_reply.started": "2025-11-01T20:53:35.867181Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved unstructured ckpt: unstructured_outputs/generated/ckpt_s30.pth (achieved sparsity 0.3000)\n",
      "Saved unstructured ckpt: unstructured_outputs/generated/ckpt_s50.pth (achieved sparsity 0.5000)\n",
      "Saved unstructured ckpt: unstructured_outputs/generated/ckpt_s70.pth (achieved sparsity 0.7000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.3, 'unstructured_outputs/generated/ckpt_s30.pth', 0.29999999844719416),\n",
       " (0.5, 'unstructured_outputs/generated/ckpt_s50.pth', 0.5000000077640292),\n",
       " (0.7, 'unstructured_outputs/generated/ckpt_s70.pth', 0.6999999937887766)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "from collections import OrderedDict\n",
    "from copy import deepcopy\n",
    "\n",
    "def generate_unstructured_pruned_checkpoints(baseline_ckpt, sparsity_list, out_dir='unstructured_outputs/generated', device='cpu'):\n",
    "\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    ck = torch.load(baseline_ckpt, map_location='cpu')\n",
    "    state = ck.get('model_state_dict', ck) if isinstance(ck, dict) else ck\n",
    "    prunable_items = [(k, v.clone()) for k, v in state.items() if k.endswith('.weight')]\n",
    "    if len(prunable_items) == 0:\n",
    "        raise RuntimeError(\"No weight tensors found in checkpoint. Check state dict keys.\")\n",
    "\n",
    "    flat_vals = []\n",
    "    mapping = []  \n",
    "    offset = 0\n",
    "    for k, v in prunable_items:\n",
    "        arr = v.detach().cpu().abs().view(-1)\n",
    "        L = arr.numel()\n",
    "        flat_vals.append(arr)\n",
    "        mapping.append((k, offset, L))\n",
    "        offset += L\n",
    "    flat_all = torch.cat(flat_vals)  # 1D\n",
    "    N_total = flat_all.numel()\n",
    "    flat_all_np = flat_all.numpy()\n",
    "\n",
    "    def make_sparsity_checkpoint(target_sparsity, fname_suffix):\n",
    "        s = float(target_sparsity)\n",
    "        if s > 1.0: s = s/100.0 if s>1.0 and s<=100.0 else s\n",
    "        target_pruned = int(math.floor(s * N_total))\n",
    "        if target_pruned <= 0:\n",
    "            new_state = {k: v.clone().cpu() for k,v in state.items()}\n",
    "            save_path = os.path.join(out_dir, f\"ckpt_us{int(round(s*100)):02d}.pth\")\n",
    "            torch.save({'model_state_dict': new_state, 'global_sparsity': 0.0}, save_path)\n",
    "            return save_path, 0.0\n",
    "\n",
    "        thresh = np.partition(flat_all_np, target_pruned-1)[target_pruned-1] if target_pruned>0 else 0.0\n",
    "        new_state = {}\n",
    "        for k, v in state.items():\n",
    "            if k.endswith('.weight'):\n",
    "                w = v.clone().cpu()\n",
    "                mask = w.abs() > float(thresh)  \n",
    "                w = w * mask\n",
    "                new_state[k] = w\n",
    "            else:\n",
    "                new_state[k] = v.clone().cpu()\n",
    "        tot = 0\n",
    "        nz = 0\n",
    "        for k,v in new_state.items():\n",
    "            if k.endswith('.weight'):\n",
    "                arr = v.numpy()\n",
    "                tot += arr.size\n",
    "                nz += np.count_nonzero(arr)\n",
    "        achieved = 1.0 - (nz / tot) if tot>0 else 0.0\n",
    "        save_path = os.path.join(out_dir, f\"ckpt_s{int(round(s*100)):02d}.pth\")\n",
    "        torch.save({'model_state_dict': new_state, 'global_sparsity': achieved}, save_path)\n",
    "        return save_path, achieved\n",
    "\n",
    "    results = []\n",
    "    for s in sparsity_list:\n",
    "        path, achieved = make_sparsity_checkpoint(s, None)\n",
    "        print(f\"Saved unstructured ckpt: {path} (achieved sparsity {achieved:.4f})\")\n",
    "        results.append((s, path, achieved))\n",
    "    return results\n",
    "\n",
    "generate_unstructured_pruned_checkpoints(\"baseline_vgg11.pth\", sparsity_list=[0.3, 0.5, 0.7])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T21:07:07.152021Z",
     "iopub.status.busy": "2025-11-01T21:07:07.151676Z",
     "iopub.status.idle": "2025-11-01T21:07:26.450909Z",
     "shell.execute_reply": "2025-11-01T21:07:26.450265Z",
     "shell.execute_reply.started": "2025-11-01T21:07:07.151996Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved structured ckpt: structured_final/generated/ckpt_s30.pth (global_sparsity=0.2765)\n",
      "Saved structured ckpt: structured_final/generated/ckpt_s50.pth (global_sparsity=0.4526)\n",
      "Saved structured ckpt: structured_final/generated/ckpt_s70.pth (global_sparsity=0.6229)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.3, 'structured_final/generated/ckpt_s30.pth', 0.2765286697805909),\n",
       " (0.5, 'structured_final/generated/ckpt_s50.pth', 0.45257312529708804),\n",
       " (0.7, 'structured_final/generated/ckpt_s70.pth', 0.6229331927469015)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_structured_pruned_checkpoints(baseline_ckpt, sparsity_list, out_dir='structured_final/generated', device='cpu'):\n",
    "  \n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    ck = torch.load(baseline_ckpt, map_location='cpu')\n",
    "    state = ck.get('model_state_dict', ck) if isinstance(ck, dict) else ck\n",
    "    try:\n",
    "        vgg_model.load_state_dict(state)\n",
    "    except Exception:\n",
    "        vgg_model.load_state_dict(state, strict=False)\n",
    "    vgg_model.to(device)\n",
    "    vgg_model.eval()\n",
    "\n",
    "    convs = collect_convs_from_global_vgg() \n",
    "\n",
    "    saved = []\n",
    "    for s in sparsity_list:\n",
    "        sp = float(s)\n",
    "        if sp>1.0: sp = sp/100.0\n",
    "        keep_frac = max(0.0, 1.0 - sp)\n",
    "        kept_map = {}\n",
    "        for name, conv in convs:\n",
    "            kept_map[name] = choose_kept_channels(conv, keep_frac)\n",
    "        pruned_model, kept_used = build_pruned_vgg(kept_map)\n",
    "        pruned_model.to('cpu')\n",
    "        orig_params = sum(p.numel() for p in vgg_model.parameters())\n",
    "        new_params  = sum(p.numel() for p in pruned_model.parameters())\n",
    "        global_spars = 1.0 - (new_params / orig_params)\n",
    "        pruned_state = {k: v.cpu() for k,v in pruned_model.state_dict().items()}\n",
    "        save_path = os.path.join(out_dir, f\"ckpt_s{int(round(sp*100)):02d}.pth\")\n",
    "        torch.save({\n",
    "            'model_state_dict': pruned_state,\n",
    "            'kept_indices': kept_used,\n",
    "            'structured_per_layer_sparsities': {k: float(1.0 - len(v)/get_conv_out_channels(k)) for k,v in kept_used.items()} if isinstance(kept_used, dict) else None,\n",
    "            'global_sparsity': float(global_spars)\n",
    "        }, save_path)\n",
    "        print(f\"Saved structured ckpt: {save_path} (global_sparsity={global_spars:.4f})\")\n",
    "        saved.append((s, save_path, global_spars))\n",
    "    return saved\n",
    "\n",
    "def get_conv_out_channels(qname):\n",
    "    mod = get_module_by_qname(vgg_model, qname)\n",
    "    if isinstance(mod, nn.Conv2d):\n",
    "        return mod.out_channels\n",
    "    return None\n",
    "\n",
    "generate_structured_pruned_checkpoints(\"baseline_vgg11.pth\", sparsity_list=[0.3,0.5,0.7])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-01T22:10:00.170599Z",
     "iopub.status.busy": "2025-11-01T22:10:00.170086Z",
     "iopub.status.idle": "2025-11-01T22:10:42.644841Z",
     "shell.execute_reply": "2025-11-01T22:10:42.643941Z",
     "shell.execute_reply.started": "2025-11-01T22:10:00.170576Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded VGG from checkpoint 'baseline_vgg11.pth' with classifier out_features = 10\n",
      "  -> accuracy: 80.00%\n",
      "Loaded VGG from checkpoint 'unstructured_outputs/generated/ckpt_s30.pth' with classifier out_features = 10\n",
      "  -> accuracy: 79.96%\n",
      "Loaded VGG from checkpoint 'unstructured_outputs/generated/ckpt_s50.pth' with classifier out_features = 10\n",
      "  -> accuracy: 79.62%\n",
      "Loaded VGG from checkpoint 'unstructured_outputs/generated/ckpt_s70.pth' with classifier out_features = 10\n",
      "  -> accuracy: 78.36%\n",
      "Saved accuracy vs sparsity plot to acc_vs_spars_us.png\n",
      "Loaded VGG from checkpoint 'baseline_vgg11.pth' with classifier out_features = 10\n",
      "  -> accuracy: 80.00%\n",
      "  -> accuracy: 11.54%\n",
      "  -> accuracy: 10.72%\n",
      "  -> accuracy: 10.72%\n",
      "Saved accuracy vs sparsity plot to acc_vs_spars_struct.png\n"
     ]
    }
   ],
   "source": [
    "readings_us = [\n",
    "    (0.0, \"baseline_vgg11.pth\"),\n",
    "    (0.3, \"unstructured_outputs/generated/ckpt_s30.pth\"),\n",
    "    (0.5, \"unstructured_outputs/generated/ckpt_s50.pth\"),\n",
    "    (0.7, \"unstructured_outputs/generated/ckpt_s70.pth\"),\n",
    "]\n",
    "\n",
    "readings_s = [\n",
    "    (0.0, \"baseline_vgg11.pth\"),\n",
    "    (0.3, \"structured_final/generated/ckpt_s30.pth\"),\n",
    "    (0.5, \"structured_final/generated/ckpt_s50.pth\"),\n",
    "    (0.7, \"structured_final/generated/ckpt_s70.pth\"),\n",
    "]\n",
    "\n",
    "notes = accuracy_vs_sparsity_plot(readings_us, out_path='acc_vs_spars_us.png')\n",
    "notes = accuracy_vs_sparsity_plot(readings_s, out_path='acc_vs_spars_struct.png')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8627648,
     "sourceId": 13580130,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8627713,
     "sourceId": 13580213,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8627724,
     "sourceId": 13580225,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8628138,
     "sourceId": 13580801,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
